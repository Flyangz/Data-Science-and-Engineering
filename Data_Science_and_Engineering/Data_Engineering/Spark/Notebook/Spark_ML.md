# Part VI. Advanced Analytics and MachineÂ Learning

---

## Advanced Analytics and Machine LearningÂ Overview

### 1.A Short Primer on Advanced Analytics

**ç›®çš„**ï¼šderiving insights and making predictions or recommendations

**æ¦‚å¿µ**

Supervised Learningï¼šç”¨å«æœ‰labelï¼ˆå› å˜é‡ï¼‰çš„å†å²æ•°æ®è®­ç»ƒæ¨¡å‹æ¥é¢„æµ‹æ–°æ•°æ®çš„labelã€‚è®­ç»ƒè¿‡ç¨‹é€šå¸¸ç”¨GDæ¥ä¸æ–­è°ƒæ•´å‚æ•°ä»¥å®Œå–„æ¨¡å‹ã€‚

Classificationï¼šé¢„æµ‹ä¸€ä¸ª*categorical* å˜é‡ï¼Œå³ä¸€ä¸ªç¦»æ•£çš„ï¼Œæœ‰é™çš„valueé›†åˆã€‚ç»“æœåªæœ‰ä¸€ä¸ªå€¼æ—¶ï¼Œæ ¹æ®*categorical*å˜é‡çš„å¯å–å€¼çš„æ•°é‡åˆ†ä¸ºbinaryå’Œmulticlassã€‚ç»“æœæœ‰å¤šä¸ªå€¼ä¸ºMultilabel Classification

Regressionï¼šé¢„æµ‹ä¸€ä¸ªè¿ç»­å˜é‡ï¼Œä¸€ä¸ªå®æ•°ã€‚

Recommendationï¼šåŸºäºç›¸ä¼¼å®¢æˆ·çš„å–œå¥½æˆ–ç›¸ä¼¼å•†å“æ¥æ¨è

Unsupervised Learningï¼šä»æ•°æ®ä¸­å¯»æ‰¾è§„å¾‹ï¼Œæ²¡æœ‰labelã€‚

Graph Analyticsï¼šç ”ç©¶*vertices* (objects) å’Œ*edges* (relationships between those objects)ç»„æˆçš„ç»“æ„

The Advanced Analytics Process

- æ”¶é›†ç›¸å…³æ•°æ®
- Cleaning and inspecting the data to better understand it.
- ç‰¹å¾å·¥ç¨‹
- è®­ç»ƒæ¨¡å‹
- æ¯”è¾ƒå’Œè¯„ä¼°æ¨¡å‹
- åˆ©ç”¨æ¨¡å‹çš„ç»“æœæˆ–æ¨¡å‹æœ¬èº«æ¥è§£å†³é—®é¢˜

  

### 2.Sparkâ€™s Advanced Analytics Toolkit

**ä»‹ç»**

æä¾›æ¥å£å®Œæˆä¸Šè¿°Advanced Analytics Processçš„æ¨¡å—ã€‚å’Œå…¶ä»–MLåº“ç›¸æ¯”ï¼ŒSparkçš„æ›´é€‚åˆæ•°æ®é‡å¤§æ—¶ä½¿ç”¨ã€‚

> mlåº“æä¾›DFæ¥å£ã€‚æœ¬ä¹¦åªä»‹ç»å®ƒã€‚
>
> mllibåº“æ˜¯åº•å±‚APIsï¼Œç°åœ¨æ˜¯ç»´æŠ¤æ¨¡å¼ï¼Œåªä¼šä¿®å¤bugï¼Œä¸ä¼šæ·»åŠ æ–°featureã€‚ç›®å‰å¦‚æœæƒ³è¿›è¡Œstreaming trainingï¼Œåªèƒ½ç”¨millibã€‚



**æ¦‚å¿µ**

*Transformers*ï¼šè½¬æ¢æ•°æ®çš„å‡½æ•°ï¼ŒDF => æ–°DF

*Estimators*ï¼š åŒ…å«fitå’Œtransformçš„ç±»ï¼Œæ ¹æ®åŠŸèƒ½å¯åˆ†ä¸ºç”¨äºåˆå§‹åŒ–æ•°æ®çš„transformerå’Œè®­ç»ƒç®—æ³•ã€‚ 

Low-level data typesï¼š`Vector` ï¼ˆç±»ä¼¼numpyï¼‰åŒ…å«doublesç±»å‹ï¼Œå¯ä»¥sparseï¼ˆå¤§éƒ¨åˆ†ä¸º0ï¼‰æˆ–denseï¼ˆå¾ˆå¤šä¸åŒå€¼ï¼‰

```scala
//åˆ›å»ºvector
val denseVec = Vectors.dense(1.0, 2.0, 3.0)
val size = 3
val idx = Array(1,2) // locations of non-zero elements in vector
val values = Array(2.0,3.0)
val sparseVec = Vectors.sparse(size, idx, values)
sparseVec.toDense
denseVec.toSparse
```



### 3.ML in Action

> RFormula: `import org.apache.spark.ml.feature.RFormula`
>
>  â€œ~â€åˆ†å¼€targetå’Œtermsï¼›â€œ+0â€å’Œâ€œ-1â€ä¸€æ ·ï¼Œå»æ‰interceptï¼›â€œ: â€æ•°å€¼ä¹˜æ³•æˆ–äºŒè¿›åˆ¶åˆ†ç±»å€¼; â€œ.â€é™¤targetçš„æ‰€æœ‰åˆ—

```scala
//åŠ è½½æ•°æ®
var df = spark.read.json("/data/simple-ml")
df.orderBy("value2").show()

//è½¬åŒ–æ•°æ®
//è¿›å…¥è®­ç»ƒç®—æ³•çš„æ•°æ®åªèƒ½æ˜¯Double(for labels)æˆ–Vectro[Double](for features)
val supervised = new RFormula()
  .setFormula("lab ~ . + color:value1 + color:value2")
val fittedRF = supervised.fit(df)
val preparedDF = fittedRF.transform(df)
preparedDF.show()//ä¼šåœ¨åŸDFè¡¨åæ·»åŠ featureså’Œlabelåˆ—ï¼ŒSparkçš„mlç®—æ³•çš„é»˜è®¤åˆ—å

//åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
val Array(train, test) = preparedDF.randomSplit(Array(0.7, 0.3))

//è®­ç»ƒå’ŒæŸ¥çœ‹æ¨¡å‹é¢„æµ‹ç»“æœï¼ˆå¯¹è®­ç»ƒé›†çš„ï¼‰
val lr = new LogisticRegression().setLabelCol("label").setFeaturesCol("features")
println(lr.explainParams())
val fittedLR = lr.fit(train)
fittedLR.transform(train).select("label", "prediction").show()

//å»ºç«‹pipeline
//transformers or modelså®ä¾‹æ˜¯ä¸èƒ½åœ¨ä¸åŒpipelinesä¸­é‡å¤ä½¿ç”¨çš„ï¼Œæ‰€ä»¥ä¸Šé¢çš„è¦é‡æ–°new
val rForm = new RFormula()
val lr = new LogisticRegression().setLabelCol("label").setFeaturesCol("features")
val stages = Array(rForm, lr)
val pipeline = new Pipeline().setStages(stages)//pipeline.stages(0).asInstanceOf[RFormula]å¯å¾—åˆ°è¯¥stageçš„å¯¹è±¡ã€‚é€šå¸¸æ˜¯æœ€åå–modelæˆ–stringIndexerModelçš„inverse

//å»ºç«‹è¶…å‚ç½‘æ ¼
val params = new ParamGridBuilder()
  .addGrid(rForm.formula, Array(
    "lab ~ . + color:value1",
    "lab ~ . + color:value1 + color:value2"))
  .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))
  .addGrid(lr.regParam, Array(0.1, 2.0))
  .build()

//å»ºç«‹è¯„ä¼°å™¨
val evaluator = new BinaryClassificationEvaluator()
  .setMetricName("areaUnderROC")
  .setRawPredictionCol("prediction")
  .setLabelCol("label")

//äº¤å‰éªŒè¯
val tvs = new TrainValidationSplit()
  .setTrainRatio(0.75) // also the default.
  .setEstimatorParamMaps(params)
  .setEstimator(pipeline)
  .setEvaluator(evaluator)
val tvsFitted = tvs.fit(train)

//è½¬æ¢æµ‹è¯•é›†å¹¶è¯„ä¼°areaUnderROC
evaluator.evaluate(tvsFitted.transform(test))

//å¯æŸ¥çœ‹æ¨¡å‹è®­ç»ƒå†å²
val trainedPipeline = tvsFitted.bestModel.asInstanceOf[PipelineModel]
val TrainedLR = trainedPipeline.stages(1).asInstanceOf[LogisticRegressionModel]
val summaryLR = TrainedLR.summary
summaryLR.objectiveHistory
//æå–æœ€ä¼˜æ¨¡å‹çš„å‚æ•°
.stages.last.extractParamMap

//æ¯ä¸ªæ¨¡å‹çš„å¾—åˆ†å’Œå‚æ•°
val paramsAndMetrics = tvsFitted.validationMetrics.
  zip(tvsFitted.getEstimatorParamMaps).sortBy(-_._1)
paramsAndMetrics.foreach { case (metric, params) => 
    println(metric)
    println(params)
    println() 
}


//å‚¨å­˜æ¨¡å‹
tvsFitted.write.overwrite().save("path")
//åŠ è½½æ¨¡å‹ï¼Œæ ¹æ®æ¨¡å‹ç‰ˆæœ¬æ¥loadï¼Œè¿™é‡Œloadçš„æ˜¯CrossValidatorå¾—åˆ°çš„æ¨¡å‹ï¼Œæ‰€ä»¥ç”¨CrossValidatorModelï¼ˆè¿™é‡ŒTrainValidationSplitModelï¼‰ã€‚ä¹‹å‰æ‰‹åŠ¨çš„LogisticRegressionåˆ™ç”¨LogisticRegressionModel
val model = TrainValidationSplitModel.load("path")
model.transform(test)
//å¦‚æœæƒ³è¾“å‡ºPMMLæ ¼å¼ï¼Œå¯ä»¥å‚è€ƒMLeapçš„github
```



### 4.éƒ¨ç½²æ¨¡å¼

- ç¦»çº¿è®­ç»ƒï¼Œç”¨äºåˆ†æï¼ˆé€‚åˆSparkï¼‰
- ç¦»çº¿è®­ç»ƒï¼Œå‚¨å­˜ç»“æœåˆ°æ•°æ®åº“ä¸­ï¼Œé€‚åˆrecommendation
- ç¦»çº¿è®­ç»ƒï¼Œå‚¨å­˜æ¨¡å‹ç”¨äºæœåŠ¡ã€‚ï¼ˆå¹¶éä½å»¶è¿Ÿï¼Œå¯åŠ¨Sparkæ¶ˆè€—å¤§ï¼‰
- å°†åˆ†å¸ƒå¼æ¨¡å‹è½¬åŒ–ä¸ºè¿è¡Œå¾—æ›´å¿«çš„å•æœºæ¨¡å¼ã€‚ï¼ˆSparkå¯å¯¼å‡ºPMMLï¼‰
- çº¿ä¸Šè®­ç»ƒå’Œä½¿ç”¨ã€‚ï¼ˆç»“åˆStructured Streamingï¼Œé€‚åˆéƒ¨åˆ†MLæ¨¡å‹ï¼‰

---



## Preprocessing and Feature Engineering

### 1.Formatting Models According to Your Use Case

- å¤§éƒ¨åˆ† classification and regressionï¼šlabelå’Œfeature
- recommendationï¼šusers, itemså’Œratings
- unsupervised learningï¼šfeatures
- graph analyticsï¼švertices DF å’Œedges DF

```scala
//4ä¸ªæ ·æœ¬æ•°æ®
val sales = spark.read.format("csv")
  .option("header", "true")
  .option("inferSchema", "true")
  .load("/data/retail-data/by-day/*.csv")
  .coalesce(5)
  .where("Description IS NOT NULL")//Sparkå¯¹nullçš„å¤„ç†è¿˜åœ¨æ”¹è¿›
val fakeIntDF = spark.read.parquet("/data/simple-ml-integers")
var simpleDF = spark.read.json("/data/simple-ml")
val scaleDF = spark.read.parquet("/data/simple-ml-scaling")
sales.cache()

//Transformersï¼Œè½¬æ¢Description
val tkn = new Tokenizer().setInputCol("Description")
tkn.transform(sales.select("Description")).show(false)
//Estimators 
val ss = new StandardScaler().setInputCol("features")
ss.fit(scaleDF).transform(scaleDF).show(false)

//High-Level Transformers
//RFormulaï¼šstringé»˜è®¤one-hotï¼ˆlabelä¸ºDoubleï¼‰ï¼Œnumericé»˜è®¤Double
val supervised = new RFormula()
  .setFormula("lab ~ . + color:value1 + color:value2")
supervised.fit(simpleDF).transform(simpleDF).show()
//SQL Transformers
val basicTransformation = new SQLTransformer()
  .setStatement("""
    SELECT sum(Quantity), count(*), CustomerID
    FROM __THIS__
    GROUP BY CustomerID
  """)
basicTransformation.transform(sales).show()
//VectorAssemblerï¼Œå°†æ‰€æœ‰featureåˆå¹¶ä¸ºä¸€ä¸ªå¤§çš„vectorï¼Œé€šå¸¸æ˜¯pipelineçš„æœ€åä¸€æ­¥
val va = new VectorAssembler().setInputCols(Array("int1", "int2", "int3"))
va.transform(fakeIntDF).show()
```



### 2.è¿ç»­å‹featureçš„è½¬æ¢

åªé€‚ç”¨äºDouble

```scala
val contDF = spark.range(20).selectExpr("cast(id as double)")

// bucketingï¼Œå‚æ•°ä¸ºæœ€å°å€¼ï¼Œè‡³å°‘ä¸‰ä¸ªä¸­é—´åˆ’åˆ†å€¼ï¼Œæœ€å¤§å€¼ï¼ˆå³ä¸€å…±æœ€å°‘6ç»„ï¼‰ã€‚ç”±äºå·²åˆ’åˆ†ï¼Œæ‰€ä»¥ä¸éœ€fit
//ä¸‹é¢åˆ†ç»„æ˜¯[-1.0,5.0),[5.0, 10.0),[10.0,250.0)...
val bucketBorders = Array(-1.0, 5.0, 10.0, 250.0, 600.0)//æœ€å€¼ä¹Ÿå¯é€‰æ‹©scala.Double.NegativeInfinity/ PositiveInfinity
val bucketer = new Bucketizer().setSplits(bucketBorders).setInputCol("id")
//å¯¹äºnull or NaN å€¼ï¼Œéœ€è¦æŒ‡å®š.handleInvalid å‚æ•°ä¸ºæŸä¸ªå€¼ï¼Œæˆ–è€…keep those values, error or null, or skip those rows

//QuantileDiscretizer
val bucketer = new QuantileDiscretizer().setNumBuckets(5).setInputCol("id")

//Scaling and Normalization
//StandardScaler,.withMeanï¼ˆé»˜è®¤falseï¼‰ï¼Œå¯¹äºsparseæ•°æ®æ¶ˆè€—å¤§
val sScaler = new StandardScaler().setInputCol("features")
//MinMaxScaler
val minMax = new MinMaxScaler().setMin(5).setMax(10).setInputCol("features")
//MaxAbsScalerï¼Œbetween âˆ’1 and 1
val maScaler = new MaxAbsScaler().setInputCol("features")
//ElementwiseProductï¼Œä¸éœ€fit
val scaleUpVec = Vectors.dense(10.0, 15.0, 20.0)
val scalingUp = new ElementwiseProduct()
  .setScalingVec(scaleUpVec)
  .setInputCol("features")//å¦‚æœfeatureçš„æŸrowæ˜¯[1, 0.1, -1]ï¼Œè½¬æ¢åå˜ä¸º[10, 1.5, -20]
//Normalizer,1,2,3ç­‰
val manhattanDistance = new Normalizer().setP(1).setInputCol("features")
```

> è¿˜æœ‰ä¸€äº›é«˜çº§çš„bucketingï¼Œå¦‚ locality sensitivity hashing (LSH) ç­‰



### 3.Categorical Features

 **recommend** re-indexing every categorical variable when pre-processing just for consistencyâ€™s sake. æ‰€ä»¥ä¸‹é¢çš„transformå’Œfitéƒ½æ˜¯ä¼ å…¥æ•´ä¸ªDFï¼Œç‰¹åˆ«æç¤ºé™¤å¤–ã€‚

```scala
//StringIndexer,ä¸€ç§string to ä¸€ä¸ªintå€¼ã€‚ä¹Ÿå¯å¯¹éstringä½¿ç”¨ï¼Œä½†ä¼šå…ˆè½¬æ¢ä¸ºstringå†è½¬ä¸ºint
val lblIndxr = new StringIndexer().setInputCol("lab").setOutputCol("labelInd")
//å¦‚æœfitåçš„transformå¯¹è±¡æœ‰æœªè§è¿‡çš„ï¼Œä¼šerrorï¼Œæˆ–è€…é€šè¿‡ä¸‹é¢ä»£ç è®¾ç½®skipæ•´ä¸ªrow
valIndexer.setHandleInvalid("skip")
valIndexer.fit(simpleDF).setHandleInvalid("skip")

//IndexToStringï¼Œä¾‹å¦‚å°†classificationçš„ç»“æœè½¬æ¢å›stringã€‚ç”±äºSparkä¿ç•™äº†å…ƒæ•°æ®ï¼Œæ‰€ä»¥ä¸éœ€è¦fitã€‚å¯èƒ½æœ‰äº›æ²¡ä¿ç•™ï¼Œå°±å¤šåŠ ä¸€æ­¥.setLabels(Model.labels)
val labelReverse = new IndexToString().setInputCol("labelInd")

//VectorIndexerï¼Œè®¾å®šæœ€å¤§åˆ†ç±»é‡ã€‚ä¸‹é¢transformerå¯¹äºuniqueå€¼å¤šäºä¸¤ä¸ªçš„ï¼Œä¸ä¼šè¿›è¡Œåˆ†ç±»ã€‚
val indxr = new VectorIndexer().setInputCol("features").setOutputCol("idxed")
  .setMaxCategories(2)

//OneHotEncoder
val lblIndxr = new StringIndexer().setInputCol("color").setOutputCol("colorInd")
val colorLab = lblIndxr.fit(simpleDF).transform(simpleDF.select("color"))
val ohe = new OneHotEncoder().setInputCol("colorInd")
ohe.transform(colorLab).show()

//æ™®é€šencoderï¼Œç”¨whenï¼Œotherwise
df.select(when(col("happy") === true, 1).otherwise(2).as("encoded")).show

//mapEncoderï¼Œä¸‹é¢å‡½æ•°å…¶å®ä¸nullæ— å…³ï¼Œä¹Ÿå¯æŸ¥æ‰¾â€œåˆ©ç”¨mapè¿›è¡Œæ•°æ®è½¬æ¢â€ï¼Œæ²¡æœ‰otherwiseå¯ç”¨ã€‚
df.na.replace("Description", Map("" -> "UNKNOWN"))
```



### 4.Text Data Transformers

ä¸¤ç±»ï¼šstring categorical variablesï¼ˆå‰é¢æåˆ°çš„ï¼‰å’Œfree-form text

```scala
//ä¸éœ€fit
//Tokenizerï¼Œspaceåˆ†éš”
val tkn = new Tokenizer().setInputCol("Description").setOutputCol("DescOut")
val tokenized = tkn.transform(sales.select("Description"))
tokenized.show(false)

//RegexTokenizerï¼Œpatternåˆ†éš”
val rt = new RegexTokenizer()
  .setInputCol("Description")
  .setOutputCol("DescOut")
  .setGaps(false)//è®¾ç½®falseå°±æå–pattern
  .setPattern(" ") // simplest expression
  .setToLowercase(true)

//StopWordsRemover
val englishStopWords = StopWordsRemover.loadDefaultStopWords("english")
val stops = new StopWordsRemover()
  .setStopWords(englishStopWords)
  .setInputCol("DescOut")

//NGramï¼Œä¸‹é¢ä»£ç å°†[Big, Data, Processing, Made]å˜ä¸º[Big Data, Data Processing, Processing Made]
val bigram = new NGram().setInputCol("DescOut").setN(2)

//è¯é¢‘CountVectorizer
val cv = new CountVectorizer()
  .setInputCol("DescOut")
  .setOutputCol("countVec")
  .setVocabSize(500)
  .setMinTF(1)//æ‰€transformçš„æ–‡æœ¬é‡Œå‡ºç°çš„æœ€ä½é¢‘ç‡
  .setMinDF(2)//æ”¶å…¥å­—å…¸çš„æœ€ä½é¢‘ç‡
//ä¸‹é¢ç»“æœï¼Œåè¾¹ç¬¬äºŒé¡¹æ˜¯å•è¯åœ¨å­—å…¸ä¸­çš„ä½ç½®ï¼ˆéå¯¹åº”ï¼‰ï¼Œç¬¬ä¸‰é¡¹ä¸ºåœ¨æ­¤rowçš„é¢‘ç‡
|[rabbit, night, light]  |(500,[150,185,212],[1.0,1.0,1.0]) |

//åè¯é¢‘HashingTFï¼Œå‡ºç°è¶Šå°‘è¯„åˆ†è¶Šé«˜ã€‚ä¸åŒäºä¸Šé¢CountVectorizerï¼ŒçŸ¥é“indexä¸èƒ½è¿”å›è¯
val tf = new HashingTF()
  .setInputCol("DescOut")
  .setOutputCol("TFOut")
  .setNumFeatures(10000)
val idf = new IDF()
  .setInputCol("TFOut")
  .setOutputCol("IDFOut")
  .setMinDocFreq(2)
idf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).show(false)
//ä¸‹é¢ç»“æœï¼Œç¬¬äºŒé¡¹ä¸ºå“ˆå¸Œå€¼ï¼Œç¬¬ä¸‰é¡¹ä¸ºè¯„åˆ†
(10000,[2591,4291,4456],[1.0116009116784799,0.0,0.0])

//Word2Vecï¼ˆæ·±åº¦å­¦ä¹ éƒ¨åˆ†ï¼Œæš‚ç•¥)
```



### 5.Feature Manipulation and Selection

```scala
//PCA
val pca = new PCA().setInputCol("features").setK(2)
pca.fit(scaleDF).transform(scaleDF).show(false)

//Interactionï¼Œç”¨RFormula 

//Polynomial Expansion
val pe = new PolynomialExpansion().setInputCol("features").setDegree(2)

//ChiSqSelector
val chisq = new ChiSqSelector()
  .setFeaturesCol("countVec")
  .setLabelCol("CustomerId")
  .setNumTopFeatures(2)//ç™¾åˆ†æ¯”
```



### 6.Advanced Topics

```scala
//Persisting Transformers
val fittedPCA = pca.fit(scaleDF)
fittedPCA.write.overwrite().save("/tmp/fittedPCA")
val loadedPCA = PCAModel.load("/tmp/fittedPCA")
loadedPCA.transform(scaleDF).show()

//è‡ªå®šä¹‰è½¬æ¢
class MyTokenizer(override val uid: String)
  extends UnaryTransformer[String, Seq[String],
    MyTokenizer] with DefaultParamsWritable {

  def this() = this(Identifiable.randomUID("myTokenizer"))

  val maxWords: IntParam = new IntParam(this, "maxWords",
    "The max number of words to return.",
  ParamValidators.gtEq(0))

  def setMaxWords(value: Int): this.type = set(maxWords, value)

  def getMaxWords: Integer = $(maxWords)

  override protected def createTransformFunc: String => Seq[String] = (
    inputString: String) => {
      inputString.split("\\s").take($(maxWords))
  }

  override protected def validateInputType(inputType: DataType): Unit = {
    require(
      inputType == StringType, s"Bad input type: $inputType. Requires String.")
  }

  override protected def outputDataType: DataType = new ArrayType(StringType,
    true)
}

// this will allow you to read it back in by using this object.
object MyTokenizer extends DefaultParamsReadable[MyTokenizer]
  val myT = new MyTokenizer().setInputCol("someCol").setMaxWords(2)
  myT.transform(Seq("hello world. This text won't show.").toDF("someCol")).show()

//å¦å¤–ä¸€ä¸ªè‡ªå®šä¹‰è½¬æ¢
class ConfigurableWordCount(override val uid: String) extends Transformer {
  final val inputCol= new Param[String](this, "inputCol", "The input column")
  final val outputCol = new Param[String](this, "outputCol", "The output column")

  def setInputCol(value: String): this.type = set(inputCol, value)

  def setOutputCol(value: String): this.type = set(outputCol, value)
  //æ„é€ å™¨
  def this() = this(Identifiable.randomUID("configurablewordcount"))
  //current stageçš„copyï¼Œä¸€èˆ¬ç”¨defaultCopyå°±å¯ä»¥äº†
  def copy(extra: ParamMap): HardCodedWordCountStage = {
    defaultCopy(extra)
  }

  //ä¿®æ”¹è¿”å›çš„schema: StructTypeã€‚è®°å¾—å…ˆæ£€æŸ¥è¾“å…¥ç±»å‹
  override def transformSchema(schema: StructType): StructType = {
    // Check that the input type is a string
    val idx = schema.fieldIndex($(inputCol))
    val field = schema.fields(idx)
    if (field.dataType != StringType) {
      throw new Exception(
        s"Input type ${field.dataType} did not match input type StringType")
    }
    // Add the return field
    schema.add(StructField($(outputCol), IntegerType, false))
  }

  def transform(df: Dataset[_]): DataFrame = {
    val wordcount = udf { in: String => in.split(" ").size }
    df.select(col("*"), wordcount(df.col($(inputCol))).as($(outputCol)))
  }
}
```

å…³äºestimatorå’Œmodelçš„è‡ªå®šä¹‰ï¼Œå‚è€ƒã€Šhigh performance sparkã€‹çš„CustomPipeline.scalaã€‚å¯ä»¥åŠ äº›cachingä»£ç ã€‚å¦å¤–org.apache.spark.ml.Predictor å’Œ org.apache.spark.ml.classificationClassifier æœ‰æ—¶æ›´æ–¹ä¾¿ï¼Œå› ä¸ºå®ƒä»¬èƒ½è‡ªåŠ¨å¤„ç†schema transformationã€‚åè€…å¤šäº†rawPredictionColumnå’ŒgetNumClassesã€‚è€Œå›å½’è·Ÿèšç±»å°±åªèƒ½ç”¨estimatoræ¥å£äº†ã€‚

---

### 7.ä¾‹å­linkage(C2)

#### 1.transpose summary table

```scala
//parsedä¸ºæŸDF
val summary = parsed.describe()//summaryè¡¨ç¬¬ä¸€è¡Œä¸ºåˆ—åï¼Œç¬¬ä¸€åˆ—ä¸ºmetricå
val schema = summary.schema//StructType(StructField(summary,StringType,true)...)å…¨éƒ¨éƒ½æ˜¯StringType
val longDF = summary.flatMap(row => {
  val metric = row.getString(0) 
  (1 until row.size).map(i => {
    (metric, schema(i).name, row.getString(i).toDouble)
    })
}).toDF("metric", "field", "value")

//å‰5è¡Œç»“æœä¸ºï¼Œä¸‹é¢è¿˜æœ‰å…¶ä»–metric
+------+------------+---------+
|metric|       field|    value|
+------+------------+---------+
| count|        id_1|5749132.0|
| count|        id_2|5749132.0|
| count|cmp_fname_c1|5748125.0|
| count|cmp_fname_c2| 103698.0|
| count|cmp_lname_c1|5749132.0|
+------+------------+---------+

//è¿›è¡Œé€è§†
val wideDF = longDF
  .groupBy("field")
  .pivot("metric")//2.2ä»¥ä¸‹è¦åŠ ï¼ŒSeq("count", "mean", "stddev", "min", "max")ï¼Ÿ
  .sum()//æ¯å¯¹ç»„åˆéƒ½æ˜¯å”¯ä¸€çš„ï¼Œæ‰€ä»¥sum()å…¶å®åªæœ‰ä¸€ä¸ªæ•°

//å¯ä»¥å°†ä¸Šé¢ä¸¤æ­¥å°è£…ä¸ºä¸€ä¸ªfunctionã€‚æ‰“å¼€IDEAï¼ˆä¸‹é¢ä»£ç å¿½ç•¥äº†import DFï¼Œsql.functionsä¹‹ç±»ï¼‰ï¼Œå†™ä¸€ä¸ªPivot.scalaï¼Œç„¶åloadè¿™ä¸ªç±»ï¼Œå°±å¯ä»¥å¯¹ä»»ä½•.describe()è¡¨ä½¿ç”¨äº†ã€‚
//è¦å¯¹ä¸Šé¢çš„summaryæ”¹ä¸ºdesc
def pivotSummary(desc: DataFrame): DataFrame = { 
    val schema = desc.schema
    import desc.sparkSession.implicits._

//æŸ¥çœ‹ç»“æœ
wideDF.select("field", "count", "mean").show()
```

> ä¸Šé¢ç»ƒä¹ ä¸­ï¼Œrow.getString(i)å¾—åˆ°çš„æ˜¯java.lang.Stringç±»ï¼Œå…¶æœ¬èº«æ²¡æœ‰toDoubleæ–¹æ³•ï¼Œæ˜¯é€šè¿‡Scalaçš„éšå¼è½¬æ¢å®ç°çš„ã€‚è¿™ç§è½¬æ¢æŠŠStringå˜ä¸ºäº†StringOpsï¼ˆScalaç±»ï¼‰ï¼Œç„¶åè°ƒç”¨è¯¥ç±»çš„toDoubleã€‚éšå¼è½¬æ¢è®©æˆ‘ä»¬å¢åŠ æ ¸å¿ƒç±»çš„åŠŸèƒ½ï¼Œä½†æœ‰æ—¶è®©äººéš¾ä»¥å¼„æ¸…åŠŸèƒ½çš„æ¥æºã€‚



#### 2.feature é€‰æ‹©

å…¶å®ç®€å•joinç”¨SQLå¯èƒ½æ›´æ¸…æ™°ï¼Œä¸‹é¢æœ‰ç‚¹é™„åŠ åŠŸèƒ½ï¼Œå½“ä½œé˜…è¯»ç†è§£ã€‚å®ƒä¸»è¦ç›®çš„æ˜¯æ¯”è¾ƒä¸¤ä¸ª.describe()è¡¨ï¼ˆä¸€ä¸ªmatchè¡¨ï¼Œä¸€ä¸ªmissè¡¨ï¼‰çš„å·®å¼‚

```scala
matchSummaryT.createOrReplaceTempView("match_desc")
missSummaryT.createOrReplaceTempView("miss_desc")
spark.sql("""
  SELECT a.field, a.count + b.count total, a.mean - b.mean delta
  FROM match_desc a INNER JOIN miss_desc b ON a.field = b.field
  WHERE a.field NOT IN ("id_1", "id_2")
  ORDER BY delta DESC, total DESC
""").show()
//ç»“æœå‰ä¸¤rowï¼Œfieldä¸­æ¯é¡¹çš„å€¼åŸŸä¸º0ï½1
+------------+---------+--------------------+
|       field|    total|               delta|
+------------+---------+--------------------+
|     cmp_plz|5736289.0|  0.9563812499852176|
|cmp_lname_c2|   2464.0|  0.8064147192926264|
+------------+---------+--------------------+
//ä¸Šè¡¨ä¸­ï¼Œtotalçœ‹æ•°æ®ç¼ºå¤±æƒ…å†µï¼Œdeltaçœ‹å·®å¼‚æƒ…å†µ
```



#### 3.äº¤å‰è¡¨åˆ†æ

ä¾‹å­æ•°æ®æ¯”è¾ƒç®€å•ï¼Œæ‰€ä»¥ä¹¦ä¸­æ–¹æ³•åªæ˜¯ä¸ºäº†å±•ç¤ºï¼Œæœ€åæœ‰ç®€ä¾¿çš„æ–¹æ³•å®ç°ã€‚

åˆ›å»ºcase classå¹¶æŠŠDF[row] -> Dataset[MatchData]ï¼Œè¿™æ ·å¯¹ç»“æ„åŒ–è¡¨æ ¼ä¸­çš„å…ƒç´ çš„æ“ä½œæ›´çµæ´»ï¼Œä½†ä¼šæ”¾å¼ƒDFçš„éƒ¨åˆ†æ•ˆç‡ã€‚

```scala
//1.åˆ›å»ºè½¬åŒ–éœ€è¦çš„ç±»
case class MatchData(//ä¸‹é¢åˆ å»äº†éƒ¨åˆ†å˜é‡
    id_1: Int,
    cmp_fname_c1: Option[Double], 
    cmp_plz: Option[Int], 
    is_match: Boolean
)
val matchData = parsed.as[MatchData]

//2.åˆ›å»ºcase classæ‹¥æœ‰+æ–¹æ³•
case class Score(value: Double) { 
    def +(oi: Option[Int]) = {
		Score(value + oi.getOrElse(0)) 
    }
}
//3.æŠŠè®¤ä¸ºåˆé€‚çš„featureåŠ æ€»ï¼ˆåˆ†ææ¥è‡ªSpark Join1.å®è·µï¼‰ï¼Œå¾—å‡ºè¯„åˆ†
def scoreMatchData(md: MatchData): Double = {
    (Score(md.cmp_lname_c1.getOrElse(0.0)) + md.cmp_plz +
     	md.cmp_by + md.cmp_bd + md.cmp_bm).value
}
//4.æŠŠè¯„åˆ†å’ŒlabelæŠ½å‡ºæ¥
val scored = matchData.map { 
    md => (scoreMatchData(md), md.is_match) 
}.toDF("score", "is_match")

//5.åˆ›å»ºcrosstab
def crossTabs(scored: DataFrame, t: Double): DataFrame = { 
    scored.selectExpr(s"score >= $t as above", "is_match")
          .groupBy("above")
          .pivot("is_match")
          .count()
}

crossTabs(scored, 2.0).show()
+-----+-----+-------+ 
|above| true|  false| 
+-----+-----+-------+
| true|20931| 596414| 
|false| null|5131787| 
+-----+-----+-------+

//ä¸Šé¢å¯ä»¥ç›´æ¥åœ¨DFå®ç°ï¼Œè€Œä¸éœ€è¦è½¬ä¸ºDatasetã€‚
val scored = parsed
    .na.fill(0, Seq("cmp_lname_c1",...))//å¦‚æœä¸fillï¼Œä¸‹é¢åˆ—ä¸­æœ‰nullè¡Œçš„ç»“æœä¸ºnull
    .withColumn("score", expr("cmp_lname_c1 + ..."))
    .select("score", "is_match")
```

------



## 

## Classification

#### 1.å››ä¸ªæœ€å¸¸ç”¨æ¨¡å‹

**æ¨¡å‹Scalability**

| Model                  | Features count  | Training examples | Output classes                  |
| ---------------------- | --------------- | ----------------- | ------------------------------- |
| Logistic regression    | 1 to 10 million | No limit          | Features x Classes < 10 million |
| Decision trees         | 1,000s          | No limit          | Features x Classes < 10,000s    |
| Random forest          | 10,000s         | No limit          | Features x Classes < 100,000s   |
| Gradient-boosted trees | 1,000s          | No limit          | Features x Classes < 10,000s    |

**æ¨¡å‹å‚æ•°**

| Logistic regression                              | Decision trees                                               | Random forest                                            |     GDBTï¼ˆç›®å‰åªèƒ½binaryï¼‰     |
| ------------------------------------------------ | ------------------------------------------------------------ | -------------------------------------------------------- | :----------------------------: |
| family: multinomial or binary                    | maxDepth: é»˜è®¤5                                              | numTrees                                                 | lossType: åªæ”¯æŒ logistic loss |
| elasticNetParam: 0~1, 0ä¸ºçº¯L2ï¼Œ1ä¸ºçº¯L1           | maxBins: å¯¹æŸfeatureçš„åˆ†ç±»æ•°ï¼Œé»˜è®¤32                         | featureSubsetStrategyç‰¹å¾è€ƒè™‘æ•°: auto, all, sqrt, log2ç­‰ |         maxIter:  100          |
| fitIntercept: boolean å¦‚æœæ²¡æœ‰normalizedé€šå¸¸ä¼šè®¾ | impurity:   â€œentropyâ€ or â€œginiâ€ (default)                    |                                                          |     stepSize: 0~1ï¼Œé»˜è®¤0.1     |
| regParam: >=0                                    | minInfoGain: æœ€å° information gainï¼Œé»˜è®¤0                    |                                                          |                                |
| standardization: boolean                         | minInstancePerNodeï¼šé»˜è®¤1                                    |                                                          |                                |
|                                                  |                                                              |                                                          |                                |
| maxIter: é»˜è®¤100ï¼Œä¸åº”è¯¥ç¬¬ä¸€ä¸ªè°ƒ                 | checkpointInterval: -1å–æ¶ˆï¼Œ10è¡¨ç¤ºæ¯10æ¬¡è¿­ä»£è®°å½•ä¸€æ¬¡ã€‚è¿˜è¦è®¾ç½® `checkpointDirå’ŒuseNodeIdCache=true` | checkpointInterval                                       |       checkpointInterval       |
| tol: é»˜è®¤1.0E-6ï¼Œä¸åº”è¯¥ç¬¬ä¸€ä¸ªè°ƒ                  |                                                              |                                                          |                                |
| weightCol                                        |                                                              |                                                          |                                |
|                                                  |                                                              |                                                          |                                |
| threshold: 0~1ï¼Œç”¨äºæƒ©ç½šé”™è¯¯                     |                                                              |                                                          |                                |
| thresholds: åŒä¸Šï¼Œä½†é€‚ç”¨äº multiclass            | thresholds                                                   | thresholds                                               |           thresholds           |



```scala
//ä¸€äº›è¡¥å……ï¼Œä¸‹é¢ä¸»è¦æ˜¯Logistic regression
val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.3)
  .setElasticNetParam(0.8)
//æŸ¥çœ‹å‚æ•°
println(lr.explainParams())
//æŸ¥çœ‹ç»“æœï¼Œå¯¹äºmulticlassï¼Œç”¨lrModel.coefficientMatrix and lrModel.interceptVector
println(lrModel.coefficients)
println(lrModel.intercept)
//æŸ¥çœ‹summaryï¼Œæš‚æ—¶ä¸é€‚ç”¨é€»è¾‘å›å½’çš„multiclassï¼Œå…¶ä»–æ¨¡å‹ä¹Ÿå¯ä»¥å°è¯•è°ƒç”¨summaryåçœ‹æœ‰ä»€ä¹ˆæ–¹æ³•ã€‚ä¸ä¼šé‡æ–°è®¡ç®—
val summary = lrModel.summary
summary.residuals.show()//æ˜¾ç¤ºfeatureçš„æƒé‡
summary.rootMeanSquaredError
summary.r2
val bSummary = summary.asInstanceOf[BinaryLogisticRegressionSummary]
println(bSummary.areaUnderROC)
bSummary.roc.show()
bSummary.pr.show()
summary.objectiveHistory//çœ‹æ¯æ¬¡è¿­ä»£çš„æ•ˆæœ
```



#### 2.å…¶ä»–

**Naive Bayes**:

indicator variables represent the existence of a term in a document; or the *multinomial model*, where the total counts of terms are used.

æ‰€æœ‰input featuresè¦éè´Ÿ

ä¸€äº›å‚æ•°è¯´æ˜ï¼š

modelType: â€œbernoulliâ€ or â€œmultinomial"

weightCol

smoothing: é»˜è®¤1

thresholds



**Evaluators for Classification and Automating Model Tuning**

`BinaryClassificationEvaluator`: â€œareaUnderROCâ€ and areaUnderPR"

`MulticlassClassificationEvaluator:  â€œf1â€, â€œweightedPrecisionâ€, â€œweightedRecallâ€, and â€œaccuracyâ€



**Detailed Evaluation Metrics**

```scala
//ä¸‰ç§classificationç›¸ä¼¼
val out = model.transform(bInput)
  .select("prediction", "label")
  .rdd.map(x => (x(0).asInstanceOf[Double], x(1).asInstanceOf[Double]))
val metrics = new BinaryClassificationMetrics(out)
metrics.areaUnderPR
metrics.areaUnderROC
println("Receiver Operating Characteristic")
metrics.roc.toDF().show()
```



**One-vs-Rest Classifier**

æŸ¥çœ‹Spark documentationï¼Œæœ‰ä¾‹å­



### ä¾‹å­Predicting Forest Cover (C4)

å†³ç­–æ ‘å¯¹å¼‚å¸¸å€¼ï¼ˆä¸€äº›æç«¯æˆ–é”™è¯¯å€¼ï¼‰å¾ˆç¨³å¥ã€‚

æ ‘çš„å»ºç«‹å¾ˆè€—è´¹å†…å­˜ã€‚

one-hotèƒ½ä½¿æ¨¡å‹å¯¹ç‰¹å¾é€ä¸ªè€ƒè™‘ï¼ˆå½“ç„¶æ›´å å†…å­˜ï¼‰ï¼Œå¦‚æœä¸€åˆ—categoricalç‰¹å¾ï¼Œæ¨¡å‹å°±å¯èƒ½é€šè¿‡æŠŠéƒ¨åˆ†ç‰¹å¾åˆ†ç»„ï¼Œè€Œä¸ä¼šæ·±å…¥è€ƒè™‘ï¼Œä½†å‡†ç¡®ç‡ä¸ä¸€å®šæ›´å·®ã€‚

```scala
//åˆ›å»ºschema
val colNames = Seq(
    "Elevation", "Aspect", "Slope",
    ...    
  )++(
    (0 until 4).map(i => s"Wilderness_Area_$i")//æ³¨æ„è¿™ç§åˆ›å»ºcollectionçš„æ–¹å¼(IndexedSeq)
  ) ++ Seq("Cover_Type")
val data = dataWithoutHeader.toDF(colNames:_*)//æ·»åŠ schemaçš„æ–¹å¼
    .withColumn("Cover_Type", $"Cover_Type".cast("double"))

//ç”ŸæˆVector
val inputCols = trainData.columns.filter(_ != "Cover_Type") 
val assembler = new VectorAssembler().
      setInputCols(inputCols).
      setOutputCol("featureVector")
val assembledTrainData = assembler.transform(trainData)

//æ¨¡å‹
//æŸ¥çœ‹æ ‘çš„é€»è¾‘
model.toDebugString
//æ‰“å°featureImportances
model.featureImportances.toArray.zip(inputCols).
      sorted.reverse.foreach(println)

//ç”¨DFæ¥è®¡ç®—confusionMatrixï¼ŒSparkå†…ç½®çš„è¦ç”¨rdd
val confusionMatrix = predictions
  .groupBy("Cover_Type")
  .pivot("prediction", (1 to 7))//1 to 7æ˜¯å¯¹åº”predictioné‡Œé¢çš„å†…å®¹çš„ã€‚å¦‚æœpredictioné‡Œé¢æ²¡æœ‰7ï¼Œä¹Ÿå¯ä»¥ï¼Œä½†è¯¥åˆ—å…¨æ˜¯null
  .count()
  .na.fill(0.0)
  .orderBy("Cover_Type")

//è®¾è®¡ä¸€ä¸ªéšæœºclassifier
def classProbabilities(data: DataFrame): Array[Double] = { 
    val total = data.count()
    data.groupBy("Cover_Type").count()
      .orderBy("Cover_Type")
      .select("count").as[Double]
      .map(_ / total).collect() 
}
```



#### 2.å°†one-hotè½¬åŒ–ä¸ºä¸€åˆ—ç±»å‹ç‰¹å¾

```scala
def unencodeOneHot(data: DataFrame): DataFrame = {
  //è¦è½¬æ¢çš„åˆ—å
  val wildernessCols = (0 until 4).map(i => s"Wilderness_Area_$i").toArray
  //å°†ä¸Šé¢çš„åˆ—åä¸Šçš„æ•°å€¼åˆå¹¶ä¸ºä¸€ä¸ªvector
  val wildernessAssembler = new VectorAssembler()
    .setInputCols(wildernessCols)
    .setOutputCol("wilderness")
  
  //æå–1.0çš„indexæ¥å°†one-hotè½¬åŒ–categorical number
  val unhotUDF = udf((vec: Vector) => vec.toArray.indexOf(1.0).toDouble)
  
  //è½¬åŒ–æ•°æ®
  val withWilderness = wildernessAssembler.transform(data)
    .drop(wildernessCols:_*)
    .withColumn("wilderness", unhotUDF($"wilderness"))
}

//è½¬åŒ–åçš„æ•°æ®åœ¨pipelineä¸­æ·»åŠ ä¸€æ­¥indexerï¼Œè¿™æ ·èƒ½ä½¿Sparkå°†è¿™äº›èƒ½è¢«åˆ’åˆ†çš„åˆ—è§†ä¸ºcategorical featureã€‚æ³¨æ„è¿™é‡Œå‡è®¾æ‰€å¤„ç†çš„æ•°æ®ä¸­4ä¸ªç§ç±»è‡³å°‘å‡ºç°ä¸€æ¬¡ã€‚
val indexer = new VectorIndexer()
  .setMaxCategories(4)
  .setInputCol("featureVector")
  .setOutputCol("indexedVector")
val pipeline = new Pipeline().setStages(Array(assembler, indexer, classifier))
```



---



## Regression

**æ¨¡å‹Scalability**

| Model                         | Number features | Training examples |
| ----------------------------- | --------------- | ----------------- |
| Linear regression             | 1 to 10 million | No limit          |
| Generalized linear regression | 4,096           | No limit          |
| Isotonic regression           | N/A             | Millions          |
| Decision trees                | 1,000s          | No limit          |
| Random forest                 | 10,000s         | No limit          |
| Gradient-boosted trees        | 1,000s          | No limit          |
| Survival regression           | 1 to 10 million | No limit          |

**Generalized Linear Regression**

ä¸‹é¢Supported linksæŒ‡å®šçº¿æ€§é¢„æµ‹å˜é‡ä¸åˆ†å¸ƒå‡½æ•°çš„å‡å€¼ä¹‹é—´çš„å…³ç³»

| Family   | Response type            | Supported links         |
| -------- | ------------------------ | ----------------------- |
| Gaussian | Continuous               | Identity*, Log, Inverse |
| Binomial | Binary                   | Logit*, Probit, CLogLog |
| Poisson  | Count                    | Log*, Identity, Sqrt    |
| Gamma    | Continuous               | Inverse*, Idenity, Log  |
| Tweedie  | Zero-inflated continuous | Power link function     |

å‚æ•°ï¼š

familyå’Œlinkå‚è€ƒä¸Šè¡¨

 solverï¼šç›®å‰åªæ”¯æŒirlsï¼ˆiteratively reweighted least squaresï¼‰

variancePowerï¼š0~1ï¼Œ0é»˜è®¤ï¼Œ1è¡¨æ— ç©·ã€‚è¡¨ç¤ºåˆ†å¸ƒæ–¹å·®å’Œå‡å€¼çš„å…³ç³»ï¼Œåªé€‚ç”¨Tweedie 

linkPowerï¼šTweedie 

linkPredictionColï¼šboolean



**Advanced Methods**ï¼ˆç•¥ï¼‰

Survival Regression (Accelerated Failure Time)

Isotonic Regression



**Evaluators and Automating Model Tuning**

```scala
//Evaluators 
val glr = new GeneralizedLinearRegression()
  .setFamily("gaussian")
  .setLink("identity")
val pipeline = new Pipeline().setStages(Array(glr))
val params = new ParamGridBuilder().addGrid(glr.regParam, Array(0, 0.5, 1))
  .build()
val evaluator = new RegressionEvaluator()
  .setMetricName("rmse")
  .setPredictionCol("prediction")
  .setLabelCol("label")
val cv = new CrossValidator()//å¤§é‡æ•°æ®æ—¶ç”¨å¾—ä¸å¤šï¼Œå¤ªè€—æ—¶
  .setEstimator(pipeline)
  .setEvaluator(evaluator)
  .setEstimatorParamMaps(params)
  .setNumFolds(2) // should always be 3 or more but this dataset is small
val model = cv.fit(df)

//Metrics
val out = model.transform(df)
  .select("prediction", "label")
  .rdd.map(x => (x(0).asInstanceOf[Double], x(1).asInstanceOf[Double]))
val metrics = new RegressionMetrics(out)
println(s"MSE = ${metrics.meanSquaredError}")
println(s"RMSE = ${metrics.rootMeanSquaredError}")
println(s"R-squared = ${metrics.r2}")
println(s"MAE = ${metrics.meanAbsoluteError}")
println(s"Explained variance = ${metrics.explainedVariance}")
```

---



## Recommendation

### 1.Collaborative Filtering with Alternating Least Squares

ä»…æ ¹æ®ç”¨æˆ·è¿‡å»å’Œå•†å“çš„interactionæƒ…å†µï¼Œè€Œéç”¨æˆ·æˆ–å•†å“çš„attributesï¼Œæ¥ä¼°è®¡ç”¨æˆ·å¿ƒä¸­çš„å•†å“æ’åã€‚éœ€è¦ä¸‰åˆ—ï¼šuser ID , item ID, and rating ã€‚å…¶ä¸­ratingå¯æ˜¾å¼ï¼ˆuserè‡ªå·±çš„è¯„åˆ†ï¼‰å¯éšå¼ï¼ˆuserå’Œitemçš„interactionç¨‹åº¦ï¼‰ã€‚

è¯¥ç®—æ³•ä¼šå€¾å‘äºå¤§ä¼—å•†å“å’Œå…·æœ‰å¾ˆå¤šè¯´æ˜ä¿¡æ¯çš„å•†å“ã€‚å¯¹äºæ–°å•†å“æˆ–å®¢æˆ·æœ‰cold starté—®é¢˜ã€‚

åœ¨å®é™…ç”Ÿäº§å½“ä¸­ï¼Œä¸€èˆ¬ä¼šé¢„å…ˆè®¡ç®—æ‰€æœ‰ç”¨æˆ·çš„æ¨èï¼Œç„¶åå­˜åˆ°NoSQLæ¥å®ç°å®æ—¶æ¨èï¼Œä½†è¿™å¾ˆæµªè´¹å­˜å‚¨ç©ºé—´ï¼ˆå¤§éƒ¨åˆ†äººåœ¨å½“å¤©ä¸ä¸€å®šéœ€è¦æ¨èï¼‰ã€‚è€Œå•ç‹¬è®¡ç®—éœ€è¦å‡ ç§’é’Ÿçš„æ—¶é—´ã€‚[Oryx 2](https://github.com/OryxProject/oryx) å¯èƒ½æ˜¯ä¸€ä¸ªè§£å†³æ–¹æ³•ã€‚



> å°†ä¸€äº›æ•°å€¼è½¬åŒ–ä¸ºIntegeræˆ–è€…Intæ›´æœ‰æ•ˆç‡ã€‚

å‚æ•°ï¼š

rankï¼šlatent factorsæ•°é‡ï¼Œé»˜è®¤10

alphaï¼šimplicit feedbackæ—¶ï¼Œè¢«è§‚å¯Ÿå’Œæœªè¢«è§‚å¯Ÿçš„äº’åŠ¨çš„ç›¸å¯¹æ¯”é‡ï¼Œè¶Šé«˜è¯´æ˜è¶Šçœ‹é‡å·²è®°å½•çš„ï¼Œé»˜è®¤1ï¼Œ40ä¹Ÿæ˜¯ä¸ªä¸é”™çš„é€‰æ‹©

regParamï¼šé»˜è®¤0.1

implicitPrefsï¼šbooleanï¼Œæ˜¯å¦implicitï¼Œé»˜è®¤true

nonnegativeï¼šé»˜è®¤falseï¼Œnon-negative constraints on the least-squares problem

numUserBlocksï¼šé»˜è®¤10

numItemBlocksï¼šé»˜è®¤10

maxIterï¼šé»˜è®¤10

checkpointInterval

seed

coldStartStrategyï¼šè®¾å®šæ¨¡å‹å¦‚ä½•é¢„æµ‹æ–°å®¢æˆ·æˆ–å•†å“ï¼Œåªå¯é€‰`drop` and `nan`

> blocksä¸€èˆ¬æ˜¯one to five million ratings per blockï¼Œå¦‚æœå°‘äºè¿™ä¸ªæ•°ï¼Œæ›´å¤šçš„blockä¹Ÿä¸ä¼šæå‡æ•ˆç‡

```scala
val ratings = spark.read.textFile("/data/sample_movielens_ratings.txt")
  .selectExpr("split(value , '::') as col")
  .selectExpr(
    "cast(col[0] as int) as userId",
    "cast(col[1] as int) as movieId",
    "cast(col[2] as float) as rating",
    "cast(col[3] as long) as timestamp")
val Array(training, test) = ratings.randomSplit(Array(0.8, 0.2))
val als = new ALS()
  .setMaxIter(5)
  .setRegParam(0.01)
  .setUserCol("userId")
  .setItemCol("movieId")
  .setRatingCol("rating")
println(als.explainParams())
val alsModel = als.fit(training)
val predictions = alsModel.transform(test)

//ç»“æœï¼ŒæŸ¥çœ‹æ’åå‰åçš„
alsModel.recommendForAllUsers(10)
  .selectExpr("userId", "explode(recommendations)").show()
alsModel.recommendForAllItems(10)
  .selectExpr("movieId", "explode(recommendations)").show()

//è¯„ä¼°ï¼Œå…ˆå°†cold-start strategyè®¾ä¸ºdrop
val evaluator = new RegressionEvaluator()
  .setMetricName("rmse")
  .setLabelCol("rating")
  .setPredictionCol("prediction")
val rmse = evaluator.evaluate(predictions)
println(s"Root-mean-square error = $rmse")

//Regression Metrics
val regComparison = predictions.select("rating", "prediction")
  .rdd.map(x => (x.getFloat(0).toDouble,x.getFloat(1).toDouble))
val metrics = new RegressionMetrics(regComparison)
metrics.rootMeanSquaredError//å’Œä¸Šé¢Root-mean-square errorä¸€æ ·

//Ranking Metricsï¼Œä¸‹é¢è¯„ä¼°ä¸å…³æ³¨å€¼ï¼Œè€Œæ˜¯ALSæ˜¯å¦ä¼šæ¨èæŸä¸ªå€¼ä»¥ä¸Šçš„å•†å“
//ä¸‹é¢å°†ratingå¤§äº2.5çš„å®šä¸ºå¥½çš„å•†å“
val perUserActual = predictions
  .where("rating > 2.5")
  .groupBy("userId")
  .agg(expr("collect_set(movieId) as movies"))
//ä¸‹é¢æå–ALSæ¨èçš„å•†å“
val perUserPredictions = predictions
  .orderBy(col("userId"), col("prediction").desc)
  .groupBy("userId")
  .agg(expr("collect_list(movieId) as movies"))
//åˆå¹¶ä¸Šé¢ä¸¤ä¸ªæ•°æ®ï¼Œå¹¶æˆªå–æ¨èä¸­çš„å‰15ä¸ª
val perUserActualvPred = perUserActual.join(perUserPredictions, Seq("userId"))
  .map(row => (
    row(1).asInstanceOf[Seq[Integer]].toArray,
    row(2).asInstanceOf[Seq[Integer]].toArray.take(15)
  ))
//åˆ¤æ–­æ¨èçš„å¹³å‡æ­£ç¡®ç‡
val ranks = new RankingMetrics(perUserActualvPred.rdd)
ranks.meanAveragePrecision
ranks.precisionAt(5)//çœ‹å…·ä½“æ’ç¬¬5çš„å‡†ç¡®ç‡
```

> latent-factor modelsï¼šé€šè¿‡ç›¸å¯¹å°‘æ•°é‡çš„unobserved, underlying reasonsæ¥è§£é‡Šå®¢æˆ·å’Œå•†å“ä¹‹é—´å¤§é‡çš„interactionsã€‚
>
> matrix factorization modelï¼š å‡è®¾ä¸€ä¸ªç½‘æ ¼ï¼Œrowä»£è¡¨ç”¨æˆ·ï¼Œcolä»£è¡¨å•†å“ï¼Œå¦‚æœæŸæ ¼å­ä¸º1ï¼Œè¯¥æ ¼å­å¯¹åº”çš„ç”¨æˆ·å’Œå•†å“æœ‰interactionã€‚è¿™å¯ä»¥æ˜¯ä¸€ä¸ªx * yçš„çŸ©é˜µAã€‚åˆ©ç”¨latent-factor modelçš„æ€æƒ³ï¼ŒæŠŠå®¢æˆ·å’Œäº§å“é€šè¿‡äº§å“ç±»å‹ï¼ˆå¦‚ç”µå­ï¼Œå›¾ä¹¦ï¼Œé›¶é£Ÿç­‰kç§ï¼‰æ¥è”ç³»ï¼Œé‚£ä¹ˆçŸ©é˜µå¯åˆ†è§£ä¸ºx * kå’Œ y * kï¼ˆå…¶ä¸­ä¸€ä¸ªä¹˜ä»¥å¦ä¸€ä¸ªçš„è½¬ç½®å°±èƒ½å¤§æ¦‚è¿˜åŸï¼Œä½†ä¸å¯èƒ½çœŸæ­£è¿˜åŸï¼‰ã€‚ä½†æ˜¯å¦‚æœè¿™ä¸¤ä¸ªè¢«åˆ†è§£å‡ºæ¥çš„çŸ©é˜µrankå¤ªä½ï¼Œå¯¹åŸæœ¬a*bçš„è¿˜åŸå°±å¾ˆå·®ã€‚
>
> ç°åœ¨ç›®çš„æ˜¯é€šè¿‡å·²çŸ¥çš„Aï¼ˆæ‰€è®°å½•åˆ°çš„ç”¨æˆ·å’Œå•†å“çš„interactionï¼‰å’ŒYï¼ˆå•†å“çš„y * kï¼Œå®è´¨ä¹ŸæœªçŸ¥ï¼Œé€šå¸¸éšæœºç”Ÿæˆï¼‰æ±‚Xï¼ˆç”¨æˆ·çš„x * kï¼‰ã€‚ç”±äºä¸èƒ½çœŸæ­£è§£å‡ºï¼Œæ‰€ä»¥åªèƒ½è¾ƒå°‘å·®åˆ«ï¼Œè¿™å°±æ˜¯å«Least Squaresçš„åŸå› ã€‚alternating æ¥æºäºæ—¢å¯é€šè¿‡AYæ±‚Xï¼Œä¹Ÿå¯é€šè¿‡AXæ±‚Yã€‚
>
> æœ¬æ¨¡å‹çš„ user-feature å’Œ product-feature çš„çŸ©é˜µç›¸å½“å¤§ï¼ˆç”¨æˆ·æ•° x featureæ•°ï¼Œå•†å“æ•° x featureæ•°ï¼‰

### ä¾‹å­Recommending Music(C3)

### 1.textæ–‡ä»¶çš„åˆ‡å‰²

```scala
//rawUserArtistDataä¸ºRDD[String]ï¼ŒæŸrowï¼šâ€œ1000002 1 55â€
val userArtistDF = rawUserArtistData.map { line => 
    val Array(user, artist, _*) = line.split(' ') //åˆ©ç”¨_*å¯ä»¥ä½¿Arrayæ¥æ”¶å¤šäº3ä¸ªçš„å‚æ•°
    (user.toInt, artist.toInt)//å¦‚æœæ•°å€¼ä¸å¤§ï¼Œç”¨Intæ›´æœ‰æ•ˆç‡ã€‚è½¬æ¢åçœ‹minå’Œmaxç¡®å®šæ²¡æœ‰è¶…è¿‡é™åº¦ï¼Œä»¥åŠæ˜¯å¦æœ‰è´Ÿæ•°
}.toDF("user", "artist")

//åˆ†ç¦»IDå’Œå¯¹åº”çš„ä½œå“åï¼ŒæŸrowï¼šâ€œ122	appâ€
//ä¸‹é¢ä»£ç ä¸å®Œå–„ï¼Œä¸èƒ½é€‚åº”æ²¡æœ‰tabåˆ†éš”çš„rowæˆ–è€…ç©ºç™½row
rawArtistData.map { line =>
    val (id, name) = line.span(_ != '\t') //åœ¨ç¬¬ä¸€ä¸ªtabå¤„å‰æ–­å¼€ï¼ˆä¿ç•™tabï¼‰ï¼Œå¦‚æœæ²¡æœ‰tabå°±åœ¨æœ«å°¾æ–­å¼€ï¼ˆè¿”å›(String, String)ï¼Œä½†æœ€åä¸€ä¸ªæ˜¯nullï¼‰ã€‚ä¹Ÿå¯ä»¥ç”¨val Array(..) =.split("\t",2)ï¼Œ2åˆ†ä¸ºä¸¤ä»½ï¼Œ0ä¸ºå…¨åˆ†
    (id.toInt, name.trim)
}.count()

//è¿™ä¸ªæ›´å¥½ï¼ˆä¸‹é¢å¯ä»¥ç›´æ¥ç”¨Exceptionçœå»ä¸€ä¸ªifï¼Œä½†å¯èƒ½æ²¡é‚£ä¹ˆå®‰å…¨ï¼‰
val artistByID = rawArtistData.flatMap { line => 
    val (id, name) = line.span(_ != '\t')
    if (name.isEmpty) {
        None
    }else{ 
        try {
            Some(id.toInt, name.trim)//åŠ ä¸€ä¸ªä»¥ä¸Šçš„æ‹¬å·ç»“æœä¸€æ ·
        } catch {
            case _: NumberFormatException => None 
        }
    }
}.toDF("id", "name")

//æŠŠIDçš„aliasï¼ˆåˆ«åï¼‰å’Œæ­£ç¡®çš„IDè½¬åŒ–ä¸ºMapï¼ŒæŸrowï¼šâ€œ123	22â€
val artistAlias = rawArtistAlias.flatMap { line =>
    val Array(artist, alias) = line.split('\t') 
    if (artist.isEmpty) {
        None
    }else{
        Some((artist.toInt, alias.toInt))
    }
}.collect().toMap

 artistByID.filter($"id" isin (1208690, 1003926)).show()
+-------+----------------+ 
|     id|            name| 
+-------+----------------+ 
|1208690|Collective Souls| 
|1003926| Collective Soul| 
+-------+----------------+
```



### 2.åˆ©ç”¨mapè¿›è¡Œæ•°æ®è½¬æ¢

```scala
//ä¸‹é¢ä»£ç æŠŠæœ‰åˆ«åIDçš„ä½œå“ç»Ÿä¸€æˆå”¯ä¸€ID
def buildCounts(
    rawUserArtistData: Dataset[String],
    bArtistAlias: Broadcast[Map[Int,Int]]): DataFrame = {
        rawUserArtistData.map { line =>
    val Array(userID, artistID, count) = line.split(' ').map(_.toInt) 
    val finalArtistID =
        bArtistAlias.value.getOrElse(artistID, artistID)
    (userID, finalArtistID, count)
    }.toDF("user", "artist", "count")
}

val bArtistAlias = spark.sparkContext.broadcast(artistAlias) 
val trainData = buildCounts(rawUserArtistData, bArtistAlias)
trainData.cache()//è½¬æ¢åå°†ç”¨äºè®­ç»ƒçš„æ•°æ®å­˜åˆ°å†…å­˜ï¼Œå¦åˆ™ALSæ¯æ¬¡ç”¨åˆ°æ•°æ®æ—¶éƒ½è¦é‡æ–°ç®—
```



### 3.ALSæ¨¡å‹çš„å®ç°ä»¥åŠæ¨æ–­ç”¨æˆ·åå¥½

```scala
val model = new ALS()
    .setSeed(Random.nextLong())//ä¸è®¾çš„è¯ä¼šæ˜¯ç›¸åŒçš„é»˜è®¤seedï¼Œå…¶ä»–Spark MLlibç®—æ³•ä¸€æ ·
    .setImplicitPrefs(true)
    .setRank(10)
    .setRegParam(0.01)
    .setAlpha(1.0)
    .setMaxIter(5)
    .setUserCol("user")
    .setItemCol("artist")
    .setRatingCol("count")
    .setPredictionCol("prediction")
    .fit(trainData)

//çœ‹ç»“æœï¼Œä¸‹é¢ä¸¤æ®µä»£ç å½“ä½œé˜…è¯»ç†è§£
//1.æŸ¥çœ‹æŸç”¨æˆ·æ¥è§¦è¿‡çš„å•†å“
val userID = 2093760
val existingArtistIDs = trainData
    .filter($"user" === userID)
    .select("artist").as[Int].collect()
artistByID.filter($"id" isin (existingArtistIDs:_*)).show()//æ³¨æ„è¿™ä¸ª_*ç”¨æ³•
//2.å°†æ¨èçš„å•†å“æ’åï¼Œå¹¶å–å‰howManyä¸ªã€‚å¹¶æ²¡æœ‰æŠŠç”¨æˆ·æ¥è§¦è¿‡çš„å•†å“è¿‡æ»¤æ‰
def makeRecommendations( 
    model: ALSModel,
    userID: Int,
    howMany: Int): DataFrame = {
   
   val toRecommend = model.itemFactors
    .select($"id".as("artist"))
    .withColumn("user", lit(userID))
   
   model.transform(toRecommend)
    .select("artist", "prediction")
    .orderBy($"prediction".desc)
    .limit(howMany)
}
val topRecommendations = makeRecommendations(model, userID, 5) 


//åœ¨2.2ä¸­ï¼Œç›´æ¥ç”¨ä¸‹é¢ä»£ç å¯ä»¥å¾—åˆ°å…¨éƒ¨å‰10ã€‚
alsModel.recommendForAllUsers(10)
  .selectExpr("userId", "explode(recommendations)").show()

//æ ¹æ®ç»“æœæ¨æµ‹ç”¨æˆ·çš„åå¥½ï¼ˆæ ¹æ®æ‰€æ¨èå•†å“çš„åå­—ï¼‰
//æå–è¢«æ¨èçš„å•†å“çš„ID
val recommendedArtistIDs = 
    topRecommendations.select("artist").as[Int].collect()
//æŸ¥çœ‹IDå¯¹åº”çš„å•†å“å
artistByID.filter($"id" isin (recommendedArtistIDs:_*)).show()
```



### 4.ALSæ¨¡å‹çš„è¯„ä¼°

å‡è®¾interactionè¶Šå¤šï¼Œè¶Šå–œæ¬¢ã€‚å°½ç®¡ç”¨æˆ·ä¹‹å‰çš„ä¸€äº›interactionæ²¡æœ‰è¢«è®°å½•ï¼Œè€Œä¸”å°‘interactionå¹¶ä¸ä¸€å®šæ˜¯åçš„æ¨èã€‚

```scala
//ä¹¦ä¸­åˆ©ç”¨è‡ªå·±ç¼–å†™çš„mean AUCè¿›è¡Œè¯„ä¼°
//å…ˆåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶cacheã€‚ä¸‹ä¸€èŠ‚çš„CrossValidatorç»“åˆpipelineæ›´å®ç”¨ã€‚
val Array(trainData, cvData) = allData.randomSplit(Array(0.9, 0.1)) trainData.cache()
cvData.cache()
//è®¡ç®—é‡å¤§è€Œä½“ç§¯ä¸å¤§çš„å˜é‡ä¹Ÿè¦broadcastï¼Ÿ
val allArtistIDs = allData.select("artist").as[Int].distinct().collect() 
val bAllArtistIDs = spark.sparkContext.broadcast(allArtistIDs)
//é‡æ–°æ‰§è¡Œ3ä¸­çš„æ¨¡å‹

//ä¸‹é¢æ˜¯ä¹¦ä¸­è‡ªå®šä¹‰çš„æ–¹æ³•ã€‚areaUnderCurveåœ¨å…¶GitHubä¸­ã€‚
areaUnderCurve(cvData, bAllArtistIDs, model.transform)
```



### 5.è°ƒå‚

```scala
//ç”±äºæ²¡æœ‰åˆé€‚çš„evaluatorï¼Œæ²¡æœ‰è¿æˆpipelinesï¼Œè¿™é‡Œå°±æ‰‹åŠ¨å†™gridè°ƒè¶…å‚æ•°
val evaluations =
    for (rank     <- Seq(5,  30);
         regParam <- Seq(1.0, 0.0001);
         alpha    <- Seq(1.0, 40.0))
    yield {
        val model = new ALS().
        ...//å‚æ•°ç•¥

        val auc = areaUnderCurve(cvData, bAllArtistIDs, model.transform)

        model.userFactors.unpersist()//æµ‹è¯•å®Œåé©¬ä¸Šæ¸…ç©º
        model.itemFactors.unpersist()

        (auc, (rank, regParam, alpha))
    }
//æ‰“å°ç»“æœ
evaluations.sorted.reverse.foreach(println)
```

```scala
println(s"$userID -> ${recommendedArtists.mkString(", ")}")
```

> ä¾‹å­çš„ä¸€äº›è¡¥å……ï¼š
>
> æ²¡æœ‰è€ƒå¯Ÿæ•°å€¼èŒƒå›´çš„åˆç†æ€§ï¼Œä¾‹å¦‚ä¸€äº›æ’­æ”¾æ—¶é•¿è¶…è¿‡ç°å®å¯èƒ½ï¼ˆå¬æŸä¸ªartistçš„ä½œå“33å¹´æ—¶é—´ï¼‰
>
> æ²¡æœ‰å¤„ç†ç¼ºå¤±æˆ–è€…æ— æ„ä¹‰å€¼ï¼Œä¾‹å¦‚unknown artist

------





#### 2.Frequent Pattern Miningï¼ˆéœ€è¦æŸ¥çœ‹å®˜æ–¹ä¾‹å­ï¼‰

---



## Unsupervised Learning

**æ¨¡å‹Scalability**

| Model               | Statistical recommendation | Computation limits               | Training examples |
| ------------------- | -------------------------- | -------------------------------- | ----------------- |
| *k*-means           | 50 to 100 maximum          | Features x clusters < 10 million | No limit          |
| Bisecting *k*-means | 50 to 100 maximum          | Features x clusters < 10 million | No limit          |
| GMM                 | 50 to 100 maximum          | Features x clusters < 10 million | No limit          |
| LDA                 | An interpretable number    | 1,000s of topics                 | No limit          |

| k-means                                    | Bisecting *k*-means            | GMM                            | LDAï¼ˆæš‚ç•¥ï¼‰ |
| ------------------------------------------ | ------------------------------ | ------------------------------ | ----------- |
| k                                          | ğ—„                              | ğ—„                              | ğ—„ï¼šé»˜è®¤10   |
| initModeï¼šrandom and é»˜è®¤ğ˜¬-means\|\|       | minDivisibleClusterSizeï¼šé»˜è®¤1 |                                |             |
| initStepsï¼šé»˜è®¤2ï¼Œğ˜¬-means\|\| åˆå§‹åŒ–çš„æ­¥æ•° |                                |                                |             |
| maxIterï¼šé»˜è®¤20                            | maxIterï¼šé»˜è®¤20                | maxIterï¼šé»˜è®¤100               |             |
| tolï¼šé»˜è®¤0.001ï¼ˆè¶Šå°å¯ä»¥ç§»åŠ¨å¾—è¶Šå¤šï¼‰       |                                | tolï¼šé»˜è®¤0.01ï¼Œä¼šå—maxIteré™åˆ¶ |             |

### Anomaly Detection in Network Traffic (C5)

```scala
//æŸ¥çœ‹clusterå¯¹labelçš„åˆ†ç»„æƒ…å†µ
val withCluster = pipelineModel.transform(numericOnly)
withCluster.select("cluster", "label").
    groupBy("cluster", "label").count().
    orderBy($"cluster", $"count".desc).
    show(25)
```

#### 1.è¯„ä¼°KMeanï¼ˆæ‹ç‚¹æˆ–Entropyï¼‰

```scala
//ç”±äºéç›‘ç£å­¦ä¹ ç¼ºä¹evaluatorï¼Œæ‰€ä»¥ç½‘æ ¼ä¹Ÿåªèƒ½æ‰‹åŠ¨ç®—
def clusteringScore0(data: DataFrame, k: Int): Double = { 
    val assembler = new VectorAssembler()
       .setInputCols(data.columns.filter(_ != "label"))
       .setOutputCol("featureVector") 
    val kmeans = new KMeans()
       .setSeed(Random.nextLong())
       .setK(k)
       .setPredictionCol("cluster")
       .setFeaturesCol("featureVector")
    val pipeline = new Pipeline().setStages(Array(assembler, kmeans))
    val kmeansModel = pipeline.fit(data).stages.last.asInstanceOf[KMeansModel]   
    kmeansModel.computeCost(assembler.transform(data)) / data.count()
}

//åœ¨æ‹ç‚¹æ‰¾å‡ºåˆé€‚çš„K
(20 to 100 by 20).map(k => (k, clusteringScore0(numericOnly, k))).
      foreach(println)

//å®šä¹‰entropyçš„è®¡ç®—æ–¹æ³•
def entropy(counts: Iterable[Int]): Double = { 
    val values = counts.filter(_ > 0)
    val n = values.map(_.toDouble).sum 
    values.map { v => 
        val p=v/n
        -p * math.log(p)
    }.sum
}

//è®¡ç®—Entropyè¯„åˆ†
val clusterLabel = pipelineModel.transform(data). 
    select("cluster", "label").as[(Int, String)]
val weightedClusterEntropy = clusterLabel. 
    groupByKey { case (cluster, _) => cluster }. 
    mapGroups { case (_, clusterLabels) =>
        val labels = clusterLabels.map { case (_, label) => label }.toSeq 
        val labelCounts = labels.groupBy(identity).values.map(_.size)    
        labels.size * entropy(labelCounts)
    }.collect()

weightedClusterEntropy.sum / data.count()
```



#### 2.åˆ†ç±»å˜é‡è½¬ä¸ºone-hot

```scala
//nonnumeric features ä¸èƒ½ç”¨äºKMeanï¼Œå¯å°†categorical featuresè½¬ä¸ºone-hot
//å¯ä»¥ç›´æ¥åœ¨å‡½æ•°ä¸ŠåŠ colå‚æ•°ï¼Œè€Œä¸éœ€è¦æŒ‡æ˜è¡¨...
def oneHotPipeline(inputCol: String): (Pipeline, String) = { 
    val indexer = new StringIndexer().
        setInputCol(inputCol).
        setOutputCol(inputCol + "_indexed") 
    val encoder = new OneHotEncoder().
        setInputCol(inputCol + "_indexed").
        setOutputCol(inputCol + "_vec")
    val pipeline = new Pipeline().setStages(Array(indexer, encoder))
    (pipeline, inputCol + "_vec")
}
```



#### 3.æ‰¾å‡ºå¼‚å¸¸

```scala
val clustered = pipelineModel.transform(data) 
val threshold = clustered.
    select("cluster", "scaledFeatureVector").as[(Int, Vector)].
    map { case (cluster, vec) => Vectors.sqdist(centroids(cluster), vec) }. 
    orderBy($"value".desc).take(100).last

val originalCols = data.columns
val anomalies = clustered.filter { row => 
    val cluster = row.getAs[Int]("cluster")
    val vec = row.getAs[Vector]("scaledFeatureVector")
    Vectors.sqdist(centroids(cluster), vec) >= threshold }.select(originalCols.head, originalCols.tail:_*)//select("*")
anomalies.first()
```

> æœ¬ä¾‹å­çš„è·ç¦»å‡½æ•°å¯ä»¥æ”¹ä¸ºMahalanobis distanceï¼Œä½†Sparkæš‚æ—¶æ²¡æœ‰
>
> è¿ç”¨Gaussian mixture model æˆ– DBSCAN ï¼ˆæœªå®ç°ï¼‰ä¹Ÿæ˜¯ä¸€ä¸ªé€‰æ‹©ã€‚



## MLlibï¼ˆäº†è§£ï¼‰

Mllibçš„supervisedç”¨labeled pointsï¼Œè€Œunsupervisedç”¨vectorsã€‚æ³¨æ„å®ƒä»¬ä¸åŒäºScalaå’Œspark mlçš„åŒåç±»ã€‚fromMLå¯ä»¥å°†mlçš„vectorsè½¬æ¢ä¸ºmllibçš„

```scala
import com.github.fommil.netlib.BLAS.{getInstance => blas}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.classification.{LogisticRegressionWithLBFGS,
  LogisticRegressionModel}
import org.apache.spark.mllib.linalg.{Vector => SparkVector}//æ”¹åï¼Œé¿å…æ··æ·†
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.feature._
```

### feature encoding å’Œ data preparation

Mllib æä¾›feature selectionå’Œscaling

```scala
//numericæ•°æ®
Vectors.dense()/.dense()

//å¯¹äºæ–‡æœ¬æ•°æ®
//HashingTFï¼Œå¦‚æœéœ€è¦HashingTFå¤„ç†ç»“æœå¤–çš„ä¿¡æ¯ï¼Œåº”åƒä¸‹é¢é‚£æ ·ç”¨ã€‚
def toVectorPerserving(rdd: RDD[RawPanda]): RDD[(RawPanda, SparkVector)] = {
  val ht = new HashingTF()
  rdd.map{panda =>
    val textField = panda.pt
    val tokenizedTextField = textField.split(" ").toIterable
    (panda, ht.transform(tokenizedTextField))
  }
}
//Word2Vec
def word2vecTrain(rdd: RDD[String]): Word2VecModel = {
  // Tokenize our data
  val tokenized = rdd.map(_.split(" ").toIterable)
  val wv = new Word2Vec()
  wv.fit(tokenized)
}

//å‡†å¤‡è®­ç»ƒæ•°æ®
//Supervisedï¼ŒLabeledPointæ¥æ”¶doubleå’Œvectors
LabeledPoint(booleanToDouble(rp.happy), Vectors.dense(combined))
//æ–‡å­—æ•°æ®åˆ›å»ºmap
val distinctLabels: Array[T] = rdd.distinct().collect()
 distinctLabels.zipWithIndex.map{case (label, x) => (label, x.toDouble)}.toMap
//scaling and selection
//training and prediction
//ä¿å­˜
//Saveable(internal format)
model.save()
Spercific_Model.load()
//PMMLExportableï¼ŒSparkèƒ½äº§å‡ºï¼Œä½†ä¸èƒ½ç›´æ¥è¯»å–
model.toPMML()
```

## è¡¥å……è¯´æ˜

1.å‚æ•°æœ€å¥½å…¨éƒ¨æ˜¾å¼è®¾ç½®ï¼Œä¸åŒç‰ˆæœ¬çš„é»˜è®¤å€¼å¯èƒ½å˜ã€‚

## å‚è€ƒä¹¦ç±ï¼š

OReilly Spark: The Definitive Guide

OReilly Advanced Analytics with Spark 2nd Edition