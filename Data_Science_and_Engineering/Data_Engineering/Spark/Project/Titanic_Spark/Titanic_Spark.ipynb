{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic_Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spk = org.apache.spark.sql.SparkSession@18905834\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://192.168.88.103:4040)\" target=\"new_tab\">Spark UI: local-1535716557168</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1535716557168: Some(http://192.168.88.103:4040)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spk = spark\n",
    "import spk.implicits._\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val schemaArray = StructType(Array(\n",
    "    StructField(\"PassengerId\", IntegerType, true),\n",
    "    StructField(\"Survived\", IntegerType, true),\n",
    "    StructField(\"Pclass\", IntegerType, true),\n",
    "    StructField(\"Name\", StringType, true),\n",
    "    StructField(\"Sex\", StringType, true),\n",
    "    StructField(\"Age\", FloatType, true),\n",
    "    StructField(\"SibSp\", IntegerType, true),\n",
    "    StructField(\"Parch\", IntegerType, true),\n",
    "    StructField(\"Ticket\", StringType, true),\n",
    "    StructField(\"Fare\", FloatType, true),\n",
    "    StructField(\"Cabin\", StringType, true),\n",
    "    StructField(\"Embarked\", StringType, true)\n",
    "  ))\n",
    "\n",
    "val path = \"/Titanic/train.csv\"\n",
    "val df = spark.read\n",
    "              .format(\"csv\")\n",
    "              .option(\"header\", true)\n",
    "              .schema(schemaArray)\n",
    "              .load(path)\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: float (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: float (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><td>1</td><td>0</td><td>3</td><td>Braund, Mr. Owen Harris</td><td>male</td><td>22.0</td><td>1</td><td>0</td><td>A/5 21171</td><td>7.25</td><td>NULL</td><td>S</td></tr>\n",
       "<tr><td>2</td><td>1</td><td>1</td><td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td><td>female</td><td>38.0</td><td>1</td><td>0</td><td>PC 17599</td><td>71.2833</td><td>C85</td><td>C</td></tr>\n",
       "<tr><td>3</td><td>1</td><td>3</td><td>Heikkinen, Miss. Laina</td><td>female</td><td>26.0</td><td>0</td><td>0</td><td>STON/O2. 3101282</td><td>7.925</td><td>NULL</td><td>S</td></tr>\n",
       "<tr><td>4</td><td>1</td><td>1</td><td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td><td>female</td><td>35.0</td><td>1</td><td>0</td><td>113803</td><td>53.1</td><td>C123</td><td>S</td></tr>\n",
       "<tr><td>5</td><td>0</td><td>3</td><td>Allen, Mr. William Henry</td><td>male</td><td>35.0</td><td>0</td><td>0</td><td>373450</td><td>8.05</td><td>NULL</td><td>S</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-----+-----+-----+-----------------------------------------------------+--------+------+-----+-----+------------------+---------+------+-----+\n",
       "| 1   | 0   | 3   | Braund, Mr. Owen Harris                             | male   | 22.0 | 1   | 0   | A/5 21171        | 7.25    | NULL | S   |\n",
       "| 2   | 1   | 1   | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female | 38.0 | 1   | 0   | PC 17599         | 71.2833 | C85  | C   |\n",
       "| 3   | 1   | 3   | Heikkinen, Miss. Laina                              | female | 26.0 | 0   | 0   | STON/O2. 3101282 | 7.925   | NULL | S   |\n",
       "| 4   | 1   | 1   | Futrelle, Mrs. Jacques Heath (Lily May Peel)        | female | 35.0 | 1   | 0   | 113803           | 53.1    | C123 | S   |\n",
       "| 5   | 0   | 3   | Allen, Mr. William Henry                            | male   | 35.0 | 0   | 0   | 373450           | 8.05    | NULL | S   |\n",
       "+-----+-----+-----+-----------------------------------------------------+--------+------+-----+-----+------------------+---------+------+-----+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2 = [PassengerId: int, Survived: int ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PassengerId: int, Survived: int ... 10 more fields]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Cabin，用“U”填充null，并提取Cabin的首字母\n",
    "val df2 = df.na.fill(\"U\", Seq(\"Cabin\"))\n",
    "  .withColumn(\"Cabin\", substring($\"Cabin\", 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transCabin: (df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row])org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transCabin(df: Dataset[Row]): Dataset[Row] = {\n",
    "    df.na.fill(\"U\", Seq(\"Cabin\"))\n",
    "        .withColumn(\"Cabin\", substring($\"Cabin\", 0, 1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2 = [PassengerId: int, Survived: int ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PassengerId: int, Survived: int ... 10 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 = transCabin(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Cabin|\n",
      "+-----+\n",
      "|    U|\n",
      "|    C|\n",
      "|    U|\n",
      "|    C|\n",
      "|    U|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"Cabin\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df3 = [PassengerId: int, Survived: int ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PassengerId: int, Survived: int ... 10 more fields]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Ticket, 提取船票的号码，如“A/5 21171”中的21171\n",
    "val df3 = df2.withColumn(\"Ticket\", split($\"Ticket\", \" \"))\n",
    "    .withColumn(\"Ticket\", $\"Ticket\"(size($\"Ticket\").minus(1)))\n",
    "    .filter($\"Ticket\" =!= \"LINE\")//去掉某种特殊的船票"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| Ticket|\n",
      "+-------+\n",
      "|  21171|\n",
      "|  17599|\n",
      "|3101282|\n",
      "| 113803|\n",
      "| 373450|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.select(\"Ticket\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticketTransUdf = UserDefinedFunction(<function1>,StringType,Some(List(StringType)))\n",
       "df4 = [PassengerId: int, Survived: int ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ticketTrans: (ticket: String)String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PassengerId: int, Survived: int ... 10 more fields]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Ticket, 对船票号进行分类，小于四位号码的为“1”，四位号码的以第一个数字开头，后面接上“0”，大于4位号码的，取前三个数字开头。如21171变为211\n",
    "def ticketTrans(ticket: String): String = {\n",
    "    if (ticket.length < 4) {\n",
    "        return \"1\"\n",
    "    } else if (ticket.length == 4){\n",
    "        return ticket(0)+\"0\"\n",
    "    } else {\n",
    "        return ticket.slice(0, 3)\n",
    "    }\n",
    "}\n",
    "val ticketTransUdf = udf(ticketTrans(_:String):String)\n",
    "val df4 = df3.withColumn(\"Ticket\", ticketTransUdf($\"Ticket\"))\n",
    "df4.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterList = List(245, 324, 226, 285, 453, 372, 149, 80, 340, 143, 398, 348, 231, 352, 228, 363, 362, 127, 488, 132, 211, 90, 293, 295, 172, 104, 198, 290, 229, 218, 122, 50, 173, 368, 243, 118, 214, 319, 384, 336, 240, 187, 241, 124, 235, 346, 374, 312, 267, 376, 130, 342, 263, 546, 373, 335, 116, 331, 371, 265, 284, 142, 220, 169, 386, 323, 278, 205, 219, 334, 281, 343, 272, 365, 119, 314, 341, 394, 653, 185, 270, 223, 545, 358, 234, 236, 286, 282, 297, 233, 383)\n",
       "filterList_bc = Broadcast(6)\n",
       "ticketTransAdjust = > String\n",
       "ticketTransAdjustUdf = UserDefinedFunction(<function1>,StringType,Some(List(StringType)))\n",
       "df5 = [PassengerId: ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PassengerId: int, Survived: int ... 10 more fields]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Ticket, 将数量小于等于5的类别统一归为“0”\n",
    "val filterList = df4.groupBy($\"Ticket\").count()\n",
    "  .filter($\"count\" <= 5)\n",
    "  .map(row => row.getString(0))\n",
    "  .collect.toList\n",
    "\n",
    "val filterList_bc = sc.broadcast(filterList)\n",
    "def ticketTransAdjust: (String => String) = {\n",
    "    subticket => {\n",
    "        if (filterList_bc.value.contains(subticket)) \"0\"\n",
    "            else subticket\n",
    "    }\n",
    "}\n",
    "val ticketTransAdjustUdf = udf(ticketTransAdjust)\n",
    "\n",
    "val df5 = df4.withColumn(\"Ticket\", ticketTransAdjustUdf($\"Ticket\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transTicket: (sc: org.apache.spark.SparkContext, df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row])org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transTicket(sc: SparkContext, df: Dataset[Row]): Dataset[Row] = {\n",
    "    \n",
    "//     def ticketTrans(ticket: String): String = {\n",
    "//         if (ticket.length < 4) {\n",
    "//             return \"1\"\n",
    "//         } else if (ticket.length == 4){\n",
    "//             return ticket(0)+\"0\"\n",
    "//         } else {\n",
    "//             return ticket.slice(0, 3)\n",
    "//         }\n",
    "//     }\n",
    "//     val ticketTransUdf = udf(ticketTrans(_:String):String)\n",
    "    \n",
    "    \n",
    "    \n",
    "//     def ticketTransAdjust: (String => String) = {\n",
    "//         subticket => {\n",
    "//             if (filterList_bc.value.contains(subticket)) \"0\"\n",
    "//                 else subticket\n",
    "//         }\n",
    "//     }\n",
    "//     val ticketTransAdjustUdf = udf(ticketTransAdjust)\n",
    "    \n",
    "    val ticketTransUdf = udf((ticket: String) => {\n",
    "        if (ticket.length < 4) {\n",
    "             \"1\"\n",
    "        } else if (ticket.length == 4){\n",
    "             ticket(0)+\"0\"\n",
    "        } else {\n",
    "             ticket.slice(0, 3)\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    val medDF = df.withColumn(\"Ticket\", split($\"Ticket\", \" \"))\n",
    "    .withColumn(\"Ticket\", $\"Ticket\"(size($\"Ticket\").minus(1)))\n",
    "    .filter($\"Ticket\" =!= \"LINE\")\n",
    "    .withColumn(\"Ticket\", ticketTransUdf($\"Ticket\"))\n",
    "    \n",
    "    val filterList = medDF.groupBy($\"Ticket\").count()\n",
    "  .filter($\"count\" <= 5)\n",
    "  .map(row => row.getString(0))\n",
    "  .collect.toList\n",
    "\n",
    "    val filterList_bc = sc.broadcast(filterList)\n",
    "    \n",
    "    val ticketTransAdjustUdf = udf((subticket: String) => {\n",
    "        if (filterList_bc.value.contains(subticket)) \"0\"\n",
    "        else subticket\n",
    "    })\n",
    "    \n",
    "    medDF.withColumn(\"Ticket\", ticketTransAdjustUdf($\"Ticket\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df3 = [PassengerId: int, Survived: int ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PassengerId: int, Survived: int ... 10 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df3 = transTicket(sc, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.columns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Ticket|count|\n",
      "+------+-----+\n",
      "|     0|  195|\n",
      "|    20|   97|\n",
      "|   349|   53|\n",
      "|   310|   48|\n",
      "|   347|   47|\n",
      "|   113|   47|\n",
      "|   174|   22|\n",
      "|   345|   19|\n",
      "|   175|   19|\n",
      "|   177|   18|\n",
      "|   350|   18|\n",
      "|    30|   15|\n",
      "|   315|   14|\n",
      "|   250|   14|\n",
      "|   199|   14|\n",
      "|   364|   14|\n",
      "|   176|   13|\n",
      "|   135|   13|\n",
      "|    70|   12|\n",
      "|   248|   12|\n",
      "|   370|   12|\n",
      "|    40|   12|\n",
      "|   392|   11|\n",
      "|   117|   11|\n",
      "|   330|   11|\n",
      "|   110|   10|\n",
      "|   244|   10|\n",
      "|    10|   10|\n",
      "|    60|    9|\n",
      "|     1|    9|\n",
      "|   367|    9|\n",
      "|   237|    8|\n",
      "|   239|    8|\n",
      "|   369|    8|\n",
      "|   111|    8|\n",
      "|   291|    8|\n",
      "|   230|    8|\n",
      "|   112|    8|\n",
      "|   382|    7|\n",
      "|   148|    6|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.groupBy(\"Ticket\").count().orderBy($\"count\".desc).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class org.apache.spark.SparkContext"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df6 = [PassengerId: int, Survived: int ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PassengerId: int, Survived: int ... 10 more fields]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Embarked, 用“S”填充null\n",
    "val df6 = df5.na.fill(\"S\", Seq(\"Embarked\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transEmbarked: (df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row])org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transEmbarked(df: Dataset[Row]): Dataset[Row] = {\n",
    "    df.na.fill(\"S\", Seq(\"Embarked\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df4 = [PassengerId: int, Survived: int ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PassengerId: int, Survived: int ... 10 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df4 = transEmbarked(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regex = .*, (.*?)\\..*\n",
       "titlesMap = Map(Master -> Master, Capt -> Officer, Mr -> Mr, Dr -> Officer, Don -> Royalty, Rev -> Officer, Lady -> Royalty, Mrs -> Mrs, Miss -> Miss, Mlle -> Miss, Major -> Officer, Col -> Officer, Mme -> Mrs, Sir -> Royalty, the Countess -> Royalty, Jonkheer -> Royalty, Ms -> Mrs)\n",
       "titlesMap_bc = Broadcast(7)\n",
       "df7 = [PassengerId: int, Survived: int ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PassengerId: int, Survived: int ... 11 more fields]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Name, 对头衔进行归类\n",
    "val regex = \".*, (.*?)\\\\..*\"\n",
    "\n",
    "val titlesMap = Map(\n",
    "    \"Capt\"-> \"Officer\",\n",
    "    \"Col\"-> \"Officer\",\n",
    "    \"Major\"-> \"Officer\",\n",
    "    \"Jonkheer\"-> \"Royalty\",\n",
    "    \"Don\"-> \"Royalty\",\n",
    "    \"Sir\" -> \"Royalty\",\n",
    "    \"Dr\"-> \"Officer\",\n",
    "    \"Rev\"-> \"Officer\",\n",
    "    \"the Countess\"->\"Royalty\",\n",
    "    \"Mme\"-> \"Mrs\",\n",
    "    \"Mlle\"-> \"Miss\",\n",
    "    \"Ms\"-> \"Mrs\",\n",
    "    \"Mr\" -> \"Mr\",\n",
    "    \"Mrs\" -> \"Mrs\",\n",
    "    \"Miss\" -> \"Miss\",\n",
    "    \"Master\" -> \"Master\",\n",
    "    \"Lady\" -> \"Royalty\"\n",
    ")\n",
    "\n",
    "val titlesMap_bc = sc.broadcast(titlesMap)\n",
    "\n",
    "val df7 = df6.withColumn(\"Title\", regexp_extract(($\"Name\"), regex, 1))\n",
    "  .na.replace(\"Title\", titlesMap_bc.value)\n",
    "\n",
    "// def titleAdjust: (String => String) = {\n",
    "//     subticket => titlesMap_bc.value.getOrElse(subticket, \"U\")}\n",
    "// val titleAdjustUdf = udf(titleAdjust)\n",
    "\n",
    "// val df7 = df6.select(expr(\"*\"), regexp_extract(($\"Name\"), regex, 1).alias(\"Name_clean\"))\n",
    "//     .withColumn(\"Name_final\", titleAdjustUdf($\"Name_clean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "extractTitle: (sc: org.apache.spark.SparkContext, df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row])org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extractTitle(sc: SparkContext, df: Dataset[Row]): Dataset[Row] = {\n",
    "  val regex = \".*, (.*?)\\\\..*\"\n",
    "\n",
    "  val titlesMap = Map(\n",
    "      \"Capt\"-> \"Officer\",\n",
    "      \"Col\"-> \"Officer\",\n",
    "      \"Major\"-> \"Officer\",\n",
    "      \"Jonkheer\"-> \"Royalty\",\n",
    "      \"Don\"-> \"Royalty\",\n",
    "      \"Sir\" -> \"Royalty\",\n",
    "      \"Dr\"-> \"Officer\",\n",
    "      \"Rev\"-> \"Officer\",\n",
    "      \"the Countess\"->\"Royalty\",\n",
    "      \"Mme\"-> \"Mrs\",\n",
    "      \"Mlle\"-> \"Miss\",\n",
    "      \"Ms\"-> \"Mrs\",\n",
    "      \"Mr\" -> \"Mr\",\n",
    "      \"Mrs\" -> \"Mrs\",\n",
    "      \"Miss\" -> \"Miss\",\n",
    "      \"Master\" -> \"Master\",\n",
    "      \"Lady\" -> \"Royalty\"\n",
    "  )\n",
    "\n",
    "  val titlesMap_bc = sc.broadcast(titlesMap)\n",
    "\n",
    "  df.withColumn(\"Title\", regexp_extract(($\"Name\"), regex, 1))\n",
    "    .na.replace(\"Title\", titlesMap_bc.value)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df5 = [PassengerId: int, Survived: int ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PassengerId: int, Survived: int ... 11 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df5 = extractTitle(sc, df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked, Title]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------+\n",
      "|Title|Name                                               |\n",
      "+-----+---------------------------------------------------+\n",
      "|Mr   |Braund, Mr. Owen Harris                            |\n",
      "|Mrs  |Cumings, Mrs. John Bradley (Florence Briggs Thayer)|\n",
      "|Miss |Heikkinen, Miss. Laina                             |\n",
      "|Mrs  |Futrelle, Mrs. Jacques Heath (Lily May Peel)       |\n",
      "|Mr   |Allen, Mr. William Henry                           |\n",
      "+-----+---------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select(\"Title\", \"Name\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------------+\n",
      "|concat(Title, Pclass)|          avg(Age)|\n",
      "+---------------------+------------------+\n",
      "|                Miss2|         22.390625|\n",
      "|                  Mr3|28.662222222222223|\n",
      "|                 Mrs3|33.515151515151516|\n",
      "|                Miss3|  16.1231884057971|\n",
      "|                  Mr1| 41.58045977011494|\n",
      "|              Master3| 5.350833332786958|\n",
      "|             Officer2|              42.0|\n",
      "|             Royalty1|              41.6|\n",
      "|              Master1| 5.306666672229767|\n",
      "|                 Mrs2| 33.54761904761905|\n",
      "|              Master2| 2.258888887034522|\n",
      "|                 Mrs1|              40.4|\n",
      "|                Miss1| 29.74468085106383|\n",
      "|                  Mr2| 32.76829268292683|\n",
      "|             Officer1|50.888888888888886|\n",
      "+---------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df7.groupBy(\"Pclass\", \"Title\").mean(\"Age\").select(concat($\"Title\", $\"Pclass\"), $\"avg(Age)\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df8 = [PassengerId: int, Survived: int ... 12 more fields]\n",
       "meanAgeMap = Map(Officer1 -> 50.888888888888886, Mr2 -> 32.76829268292683, Mr1 -> 41.58045977011494, Mrs1 -> 40.4, Royalty1 -> 41.6, Miss2 -> 22.390625, Mrs2 -> 33.54761904761905, Miss1 -> 29.74468085106383, Master1 -> 5.306666672229767, Officer2 -> 42.0, Miss3 -> 16.1231884057971, Master3 -> 5.350833332786958, Mrs3 -> 33.515151515151516, Master2 -> 2.258888887034522, Mr3 -> 28.662222222222223)\n",
       "meanAgeMap_bc = Broadcast(10)\n",
       "fillAge = > Double\n",
       "fillAgeUdf = UserDefinedFunction(<function1>,DoubleType,Some(List(...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function1>,DoubleType,Some(List(StringType)))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Age, 根据null age的records对应的Pclass和Name_final分组后的平均来填充缺失age。首先，生成分组key，并获取分组后的平均年龄map。然后广播map,当Age为null时，用udf返回需要填充的值。\n",
    "val df8 = df7.withColumn(\"Pclass_Title_key\", concat($\"Title\", $\"Pclass\"))\n",
    "val meanAgeMap = df8.groupBy(\"Pclass_Title_key\").mean(\"Age\").map(row => (row.getString(0), row.getDouble(1))).collect().toMap\n",
    "\n",
    "val meanAgeMap_bc = sc.broadcast(meanAgeMap)\n",
    "def fillAge: (String => Double) = {\n",
    "    comb_key => meanAgeMap_bc.value.getOrElse(comb_key, 0.0)\n",
    "    }\n",
    "val fillAgeUdf = udf(fillAge)\n",
    "                           \n",
    "val df9 = df8.withColumn(\"Age\", when($\"Age\".isNull, fillAgeUdf($\"Pclass_Title_key\")).otherwise($\"Age\"))\n",
    "\n",
    "//Age_categorized, 对Age进行分类\n",
    "import org.apache.spark.ml.feature.Bucketizer\n",
    "val ageBucketBorders = 0.0 +: (10.0 to 60.0 by 5.0).toArray :+ 150.0\n",
    "val ageBucketer = new Bucketizer().setSplits(ageBucketBorders).setInputCol(\"Age\").setOutputCol(\"Age_categorized\")\n",
    "val df10 = ageBucketer.transform(df9)\n",
    "\n",
    "//Age，用mean age填充na\n",
    "// df.groupBy(\"Sex\", \"Pclass\", \"Name_final\"\n",
    "// val meanAge = df.select(mean($\"Age\")).first.getDouble(0)\n",
    "// val df8 = df7.na.fill(meanAge, Seq(\"Age\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transAge: (sc: org.apache.spark.SparkContext, df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row])org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n",
       "categorizeAge: (df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row])org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transAge(sc: SparkContext, df: Dataset[Row]): Dataset[Row] = {\n",
    "    val medDF = df.withColumn(\"Pclass_Title_key\", concat($\"Title\", $\"Pclass\"))\n",
    "    val meanAgeMap = medDF.groupBy(\"Pclass_Title_key\").mean(\"Age\").map(row => (row.getString(0), row.getDouble(1))).collect().toMap\n",
    "    \n",
    "    val meanAgeMap_bc = sc.broadcast(meanAgeMap)\n",
    "      \n",
    "    val fillAgeUdf = udf((comb_key: String) => meanAgeMap_bc.value.getOrElse(comb_key, 0.0))\n",
    "    \n",
    "    medDF.withColumn(\"Age\", when($\"Age\".isNull, fillAgeUdf($\"Pclass_Title_key\")).otherwise($\"Age\"))\n",
    "}\n",
    "\n",
    "import org.apache.spark.ml.feature.Bucketizer\n",
    "def categorizeAge(df: Dataset[Row]): Dataset[Row] = {\n",
    "\n",
    "    val ageBucketBorders = 0.0 +: (10.0 to 60.0 by 5.0).toArray :+ 150.0\n",
    "    val ageBucketer = new Bucketizer().setSplits(ageBucketBorders).setInputCol(\"Age\").setOutputCol(\"Age_categorized\")\n",
    "    ageBucketer.transform(df).drop(\"Pclass_Title_key\")   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df6 = [PassengerId: int, Survived: int ... 12 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PassengerId: int, Survived: int ... 12 more fields]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df6 = transAge(sc, df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df7 = [PassengerId: int, Survived: int ... 12 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PassengerId: int, Survived: int ... 12 more fields]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df7 = categorizeAge(df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7.columns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked, Title, Age_categorized]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df11 = [PassengerId: int, Survived: int ... 12 more fields]\n",
       "df12 = [PassengerId: int, Survived: int ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PassengerId: int, Survived: int ... 13 more fields]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//fellow, 将SibSp和Parch相加，得出同行人数\n",
    "val df11 = df10.withColumn(\"fellow\", $\"SibSp\" + $\"Parch\").drop(\"SibSp\", \"Parch\")\n",
    "//size, 对fellow进行分类。此处其实可以留到pipeline部分一次性完成。\n",
    "val df12 = df11.withColumn(\"fellow_type\", when($\"fellow\" === 0, \"Alone\")\n",
    "                           .when($\"fellow\" <= 3, \"Small\")\n",
    "                           .otherwise(\"Large\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n",
       "createFellow: (df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row])org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n",
       "categorizeFellow: (df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row])org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def createFellow(df: Dataset[Row]): Dataset[Row] = {\n",
    "    df.withColumn(\"fellow\", $\"SibSp\" + $\"Parch\")\n",
    "}\n",
    "\n",
    "def categorizeFellow(df: Dataset[Row]): Dataset[Row] = {\n",
    "    df.withColumn(\"fellow_type\", when($\"fellow\" === 0, \"Alone\")\n",
    "                           .when($\"fellow\" <= 3, \"Small\")\n",
    "                           .otherwise(\"Large\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df8 = [PassengerId: int, Survived: int ... 13 more fields]\n",
       "df9 = [PassengerId: int, Survived: int ... 14 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PassengerId: int, Survived: int ... 14 more fields]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df8 = createFellow(df7)\n",
    "val df9 = categorizeFellow(df8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked, Title, Age_categorized, fellow, fellow_type]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df9.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|Name                                        |\n",
      "+--------------------------------------------+\n",
      "|Johnston, Mr. Andrew G                      |\n",
      "|\"Johnston, Miss. Catherine Helen \"\"Carrie\"\"\"|\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Name\").filter($\"Name\".contains(\"Johnston\")).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df13 = [PassengerId: int, Survived: int ... 15 more fields]\n",
       "femaleDiedFamily_filter = ((((Sex = female) AND (Age < 60)) AND (Survived = 0)) AND (fellow > 0))\n",
       "maleSurvivedFamily_filter = ((((Sex = male) AND (Age >= 18)) AND (Survived = 1)) AND (fellow > 1))\n",
       "df14 = [PassengerId: int, Survived: int ... 16 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PassengerId: int, Survived: int ... 16 more fields]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//FName，提取家庭名称。例如：\"Johnston, Miss. Catherine Helen \"\"Carrie\"\"\" 提取出Johnston，由于spark的读取csv时，如果有引号，读取就会出现多余的引号，所以除了split逗号，还要再split一次引号。\n",
    "val df13 = df12\n",
    "  .withColumn(\"FArray\", split($\"Name\", \",\"))\n",
    "  .withColumn(\"FName\", expr(\"FArray[0]\"))\n",
    "  .withColumn(\"FArray\", split($\"FName\", \"\\\"\"))\n",
    "  .withColumn(\"FName\", $\"FArray\"(size($\"FArray\").minus(1)))\n",
    "\n",
    "//family_type，分为三类，第一类是60岁以下女性遇难的家庭，第二类是18岁以上男性存活的家庭，第三类其他。\n",
    "val femaleDiedFamily_filter = $\"Sex\" === \"female\" and $\"Age\" < 60 and $\"Survived\" === 0 and $\"fellow\" > 0\n",
    "\n",
    "val maleSurvivedFamily_filter = $\"Sex\" === \"male\" and $\"Age\" >= 18 and $\"Survived\" === 1 and $\"fellow\" > 1\n",
    "\n",
    "val df14 = df13.withColumn(\"family_type\", when(femaleDiedFamily_filter, 1)\n",
    "                           .when(maleSurvivedFamily_filter, 2).otherwise(0))\n",
    "//familyTable，家庭分类名单，用于后续test集的转化。此处用${FName}_${family_type}的形式保存。\n",
    "df14.filter($\"family_type\".isin(1,2))\n",
    "  .select(concat($\"FName\", lit(\"_\"), $\"family_type\"))\n",
    "  .dropDuplicates()\n",
    "  .write.format(\"text\").mode(\"overwrite\").save(\"familyTable\")\n",
    "\n",
    "//如果需要直接收集成Map的话，可用下面代码。此代码先利用mapPartitions对各分块的数据进行聚合，降低直接调用count而使driver挂掉的风险。另外新建一个默认Set是为了防止某个partition并没有数据的情况（出现概率可能比较少），从而使得Set的类型变为Set[_>:Tuple]而不能直接flatten\n",
    "// val familyMap = df10\n",
    "//   .filter($\"family_type\" === 1 || $\"family_type\" === 2)\n",
    "//   .select(\"FName\", \"family_type\")\n",
    "//   .rdd\n",
    "//   .mapPartitions{iter => {\n",
    "//       if (!iter.isEmpty) {\n",
    "//       Iterator(iter.map(row => (row.getString(0), row.getInt(1))).toSet)}\n",
    "//       else Iterator(Set((\"defualt\", 9)))}\n",
    "//                 }\n",
    "//   .collect()\n",
    "//   .flatten\n",
    "//   .toMap\n",
    "\n",
    "// val l1 = df10\n",
    "//   .filter($\"family_type\" === 1)\n",
    "//   .select(\"FName\")\n",
    "//   .map(name => name.getString(0))\n",
    "//   .collect.toList\n",
    "\n",
    "// val l2 = df10\n",
    "//   .filter($\"family_type\" === 2)\n",
    "//   .select(\"FName\")\n",
    "//   .map(name => name.getString(0))\n",
    "//   .collect.toList\n",
    "\n",
    "// val familyMap = l1.map((_, 1)).++(l2.map((_, 2))).toMap\n",
    "\n",
    "// val familyMap_bc = sc.broadcast(familyMap)\n",
    "// def familyAdjust: (String => Int) = {\n",
    "//     family => familyMap_bc.value.getOrElse(family, 0)}\n",
    "// val familyAdjustUdf = udf(familyAdjust)\n",
    "\n",
    "// val df11 = df10.withColumn(\"family_type\", familyAdjustUdf($\"FName\"))\n",
    "//     .drop(\"Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "extractFName: (df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row])org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extractFName(df: Dataset[Row]): Dataset[Row] = {\n",
    "  \n",
    "    if (!df.columns.contains(\"Survived\") || !df.columns.contains(\"fellow\")){\n",
    "      throw new IllegalArgumentException(\"Check if the argument is a training set or if this training set contains column named \\\"fellow\\\"\")\n",
    "    }\n",
    "    \n",
    "    val medDF = df\n",
    "      .withColumn(\"FArray\", split($\"Name\", \",\"))\n",
    "      .withColumn(\"FName\", expr(\"FArray[0]\"))\n",
    "      .withColumn(\"FArray\", split($\"FName\", \"\\\"\"))\n",
    "      .withColumn(\"FName\", $\"FArray\"(size($\"FArray\").minus(1)))\n",
    "    \n",
    "    //family_type，分为三类，第一类是60岁以下女性遇难的家庭，第二类是18岁以上男性存活的家庭，第三类其他。\n",
    "    val femaleDiedFamily_filter = $\"Sex\" === \"female\" and $\"Age\" < 60 and $\"Survived\" === 0 and $\"fellow\" > 0\n",
    "    \n",
    "    val maleSurvivedFamily_filter = $\"Sex\" === \"male\" and $\"Age\" >= 18 and $\"Survived\" === 1 and $\"fellow\" > 1\n",
    "    \n",
    "    val resDF = medDF.withColumn(\"family_type\", when(femaleDiedFamily_filter, 1)\n",
    "                               .when(maleSurvivedFamily_filter, 2).otherwise(0))\n",
    "    //familyTable，家庭分类名单，用于后续test集的转化。此处用${FName}_${family_type}的形式保存。\n",
    "    resDF.filter($\"family_type\".isin(1,2))\n",
    "      .select(concat($\"FName\", lit(\"_\"), $\"family_type\"))\n",
    "      .dropDuplicates()\n",
    "      .write.format(\"text\").mode(\"overwrite\").save(\"familyTable\") \n",
    "      \n",
    "    resDF\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df10 = [PassengerId: int, Survived: int ... 17 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PassengerId: int, Survived: int ... 17 more fields]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df10 = extractFName(df9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                Name|   FName|\n",
      "+--------------------+--------+\n",
      "|Johnston, Mr. And...|Johnston|\n",
      "|\"Johnston, Miss. ...|Johnston|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df14.select(\"Name\", \"FName\").filter($\"FName\" === \"Johnston\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df15 = [PassengerId: int, Survived: int ... 16 more fields]\n",
       "fareBucketer = quantileDiscretizer_f99b99ba31a3\n",
       "df16 = [PassengerId: int, Survived: int ... 17 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PassengerId: int, Survived: int ... 17 more fields]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Fare。首先去掉缺失的（test集合中有一个，如果量多的话，也可以像Age那样通过头衔，年龄等因数来推断）\n",
    "val df15 = df14.na.drop(\"any\", Seq(\"Fare\"))\n",
    "\n",
    "//Fare_categorized\n",
    "import org.apache.spark.ml.feature.QuantileDiscretizer\n",
    "val fareBucketer = new QuantileDiscretizer().setNumBuckets(4).setInputCol(\"Fare\").setOutputCol(\"Fare_categorized\")\n",
    "\n",
    "val df16 = fareBucketer.fit(df15).transform(df15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transFare: (df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row])org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.QuantileDiscretizer\n",
    "def transFare(df: Dataset[Row]): Dataset[Row] = {\n",
    "    val medDF = df.na.drop(\"any\", Seq(\"Fare\"))\n",
    "    val fareBucketer = new QuantileDiscretizer().setNumBuckets(4).setInputCol(\"Fare\").setOutputCol(\"Fare_categorized\")\n",
    "    fareBucketer.fit(medDF).transform(medDF)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df11 = [PassengerId: int, Survived: int ... 18 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PassengerId: int, Survived: int ... 18 more fields]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df11 = transFare(df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked, Title, Age_categorized, fellow, fellow_type, FArray, FName, family_type, Fare_categorized]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df11.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df11.columns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prePipelineDF = [Survived: int, Pclass: int ... 9 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Survived: int, Pclass: int ... 9 more fields]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val prePipelineDF = df11.select(\"Survived\", \"Pclass\", \"Sex\", \"Age_categorized\", \"fellow_type\", \"Fare_categorized\", \"Embarked\", \"Cabin\", \"Ticket\", \"Title\", \"family_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Survived, Pclass, Sex, Age_categorized, fellow_type, Fare_categorized, Embarked, Cabin, Ticket, Title, family_type]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prePipelineDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,3,male,3.0,Small,0.0,S,U,0,Mr,0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prePipelineDF.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+---------------+-----------+----------------+--------+-----+------+-----+-----------+\n",
      "|Survived|Pclass| Sex|Age_categorized|fellow_type|Fare_categorized|Embarked|Cabin|Ticket|Title|family_type|\n",
      "+--------+------+----+---------------+-----------+----------------+--------+-----+------+-----+-----------+\n",
      "|       0|     3|male|            3.0|      Small|             0.0|       S|    U|     0|   Mr|          0|\n",
      "+--------+------+----+---------------+-----------+----------------+--------+-----+------+-----+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prePipelineDF.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建pipeline，包括转换数据的格式，如double，onehot等，划分label和feature，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stringCols = Array(Sex, fellow_type, Embarked, Cabin, Ticket, Title)\n",
       "subOneHotCols = Array(Sex_index, fellow_type_index, Embarked_index, Cabin_index, Ticket_index, Title_index)\n",
       "index_transformers = Array(strIdx_e325b470f2a7, strIdx_1afd4e18f157, strIdx_f35f19bed306, strIdx_c65c155911a2, strIdx_3c51a6aaf946, strIdx_e1a2e89d57d1)\n",
       "oneHotCols = Array(Sex_index, fellow_type_index, Embarked_index, Cabin_index, Ticket_index, Title_index, Pclass, Age_categorized, Fare_categorized, family_type)\n",
       "vectorCols = Array(Sex_index_encoded, fellow_type_index_encoded, Embarked_index_encoded, Cabin_index_encod...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Sex_index_encoded, fellow_type_index_encoded, Embarked_index_encoded, Cabin_index_encoded, Ticket_index_encoded, Title_index_encoded, Pclass_encoded, Age_categorized_encoded, Fare_categorized_encoded, family_type_encoded]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// +--------+------+----+---------------+-----------+----------------+--------+-----+------+-----+-----------+\n",
    "// |Survived|Pclass| Sex|Age_categorized|fellow_type|Fare_categorized|Embarked|Cabin|Ticket|Title|family_type|\n",
    "// +--------+------+----+---------------+-----------+----------------+--------+-----+------+-----+-----------+\n",
    "// |       0|     3|male|            3.0|      Small|             0.0|       S|    U|     0|   Mr|          0|\n",
    "// +--------+------+----+---------------+-----------+----------------+--------+-----+------+-----+-----------+\n",
    "\n",
    "prePipelineDF.cache()\n",
    "\n",
    "//StringIndexer and OneHotEncoder pipeline\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "val stringCols = Array(\"Sex\",\"fellow_type\", \"Embarked\", \"Cabin\", \"Ticket\", \"Title\")\n",
    "val subOneHotCols = stringCols.map(cname => s\"${cname}_index\")\n",
    "val index_transformers: Array[org.apache.spark.ml.PipelineStage] = stringCols.map(\n",
    "  cname => new StringIndexer()\n",
    "    .setInputCol(cname)\n",
    "    .setOutputCol(s\"${cname}_index\")\n",
    "    .setHandleInvalid(\"skip\")\n",
    ")\n",
    "\n",
    "import org.apache.spark.ml.feature.OneHotEncoder\n",
    "val oneHotCols = subOneHotCols ++ Array(\"Pclass\", \"Age_categorized\", \"Fare_categorized\", \"family_type\")\n",
    "val vectorCols = oneHotCols.map(cname => s\"${cname}_encoded\")\n",
    "val encode_transformers: Array[org.apache.spark.ml.PipelineStage] = oneHotCols.map(\n",
    "  cname => new OneHotEncoder()\n",
    "    .setInputCol(cname)\n",
    "    .setOutputCol(s\"${cname}_encoded\")\n",
    ")\n",
    "\n",
    "val pipelineStage = index_transformers ++ encode_transformers\n",
    "import org.apache.spark.ml.Pipeline\n",
    "val index_onehot_pipeline = new Pipeline().setStages(pipelineStage)\n",
    "val index_onehot_pipelineModel = index_onehot_pipeline.fit(prePipelineDF)\n",
    "val df_indexed = pipelineModel.transform(prePipelineDF)\n",
    "\n",
    "df_indexed.cache()\n",
    "\n",
    "//separate and model pipeline，包含划分label和features，机器学习模型的pipeline\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "val vectorAssembler = new VectorAssembler()\n",
    "  .setInputCols(vectorCols)\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "val rfc = new RandomForestClassifier()\n",
    "    .setLabelCol(\"Survived\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "\n",
    "import org.apache.spark.ml.classification.GBTClassifier\n",
    "val gbtc = new GBTClassifier()\n",
    "    .setLabelCol(\"Survived\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "\n",
    "import org.apache.spark.ml.Pipeline\n",
    "val pipeline = new Pipeline().setStages(Array(vectorAssembler, gbtc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index_onehot: (df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row])(org.apache.spark.sql.Dataset[org.apache.spark.sql.Row], Array[String])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.OneHotEncoder\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "def index_onehot(df: Dataset[Row]): Tuple2[Dataset[Row], Array[String]] = {\n",
    "    val stringCols = Array(\"Sex\",\"fellow_type\", \"Embarked\", \"Cabin\", \"Ticket\", \"Title\")\n",
    "    val subOneHotCols = stringCols.map(cname => s\"${cname}_index\")\n",
    "    val index_transformers: Array[org.apache.spark.ml.PipelineStage] = stringCols.map(\n",
    "      cname => new StringIndexer()\n",
    "        .setInputCol(cname)\n",
    "        .setOutputCol(s\"${cname}_index\")\n",
    "        .setHandleInvalid(\"skip\")\n",
    "    )\n",
    "    \n",
    "    val oneHotCols = subOneHotCols ++ Array(\"Pclass\", \"Age_categorized\", \"Fare_categorized\", \"family_type\")\n",
    "    val vectorCols = oneHotCols.map(cname => s\"${cname}_encoded\")\n",
    "    val encode_transformers: Array[org.apache.spark.ml.PipelineStage] = oneHotCols.map(\n",
    "      cname => new OneHotEncoder()\n",
    "        .setInputCol(cname)\n",
    "        .setOutputCol(s\"${cname}_encoded\")\n",
    "    )\n",
    "    \n",
    "    val pipelineStage = index_transformers ++ encode_transformers\n",
    "    val index_onehot_pipeline = new Pipeline().setStages(pipelineStage)\n",
    "    val index_onehot_pipelineModel = index_onehot_pipeline.fit(df)\n",
    "    \n",
    "    val resDF = index_onehot_pipelineModel.transform(df).drop(stringCols:_*).drop(subOneHotCols:_*)\n",
    "    (resDF, vectorCols)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_indexed = [Survived: int, Pclass: int ... 13 more fields]\n",
       "cols = Array(Sex_index_encoded, fellow_type_index_encoded, Embarked_index_encoded, Cabin_index_encoded, Ticket_index_encoded, Title_index_encoded, Pclass_encoded, Age_categorized_encoded, Fare_categorized_encoded, family_type_encoded)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Survived: int, Pclass: int ... 13 more fields]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (df_indexed, cols) = index_onehot(prePipelineDF)\n",
    "df_indexed.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Survived, Pclass, Age_categorized, Fare_categorized, family_type, Sex_index_encoded, fellow_type_index_encoded, Embarked_index_encoded, Cabin_index_encoded, Ticket_index_encoded, Title_index_encoded, Pclass_encoded, Age_categorized_encoded, Fare_categorized_encoded, family_type_encoded]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_indexed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_indexed.columns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><td>(1,[0],[1.0])</td><td>(2,[1],[1.0])</td><td>(2,[0],[1.0])</td><td>(8,[0],[1.0])</td><td>(39,[0],[1.0])</td><td>(5,[0],[1.0])</td><td>(3,[],[])</td><td>(11,[3],[1.0])</td><td>(3,[0],[1.0])</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>(1,[],[])</td><td>(2,[1],[1.0])</td><td>(2,[1],[1.0])</td><td>(8,[1],[1.0])</td><td>(39,[7],[1.0])</td><td>(5,[2],[1.0])</td><td>(3,[1],[1.0])</td><td>(11,[6],[1.0])</td><td>(3,[],[])</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>(1,[],[])</td><td>(2,[0],[1.0])</td><td>(2,[0],[1.0])</td><td>(8,[0],[1.0])</td><td>(39,[3],[1.0])</td><td>(5,[1],[1.0])</td><td>(3,[],[])</td><td>(11,[4],[1.0])</td><td>(3,[1],[1.0])</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>(1,[],[])</td><td>(2,[1],[1.0])</td><td>(2,[0],[1.0])</td><td>(8,[1],[1.0])</td><td>(39,[4],[1.0])</td><td>(5,[2],[1.0])</td><td>(3,[1],[1.0])</td><td>(11,[6],[1.0])</td><td>(3,[],[])</td><td>(2,[0],[1.0])</td></tr>\n",
       "<tr><td>(1,[0],[1.0])</td><td>(2,[0],[1.0])</td><td>(2,[0],[1.0])</td><td>(8,[0],[1.0])</td><td>(39,[0],[1.0])</td><td>(5,[0],[1.0])</td><td>(3,[],[])</td><td>(11,[6],[1.0])</td><td>(3,[1],[1.0])</td><td>(2,[0],[1.0])</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------------+---------------+---------------+---------------+----------------+---------------+---------------+----------------+---------------+---------------+\n",
       "| (1,[0],[1.0]) | (2,[1],[1.0]) | (2,[0],[1.0]) | (8,[0],[1.0]) | (39,[0],[1.0]) | (5,[0],[1.0]) | (3,[],[])     | (11,[3],[1.0]) | (3,[0],[1.0]) | (2,[0],[1.0]) |\n",
       "| (1,[],[])     | (2,[1],[1.0]) | (2,[1],[1.0]) | (8,[1],[1.0]) | (39,[7],[1.0]) | (5,[2],[1.0]) | (3,[1],[1.0]) | (11,[6],[1.0]) | (3,[],[])     | (2,[0],[1.0]) |\n",
       "| (1,[],[])     | (2,[0],[1.0]) | (2,[0],[1.0]) | (8,[0],[1.0]) | (39,[3],[1.0]) | (5,[1],[1.0]) | (3,[],[])     | (11,[4],[1.0]) | (3,[1],[1.0]) | (2,[0],[1.0]) |\n",
       "| (1,[],[])     | (2,[1],[1.0]) | (2,[0],[1.0]) | (8,[1],[1.0]) | (39,[4],[1.0]) | (5,[2],[1.0]) | (3,[1],[1.0]) | (11,[6],[1.0]) | (3,[],[])     | (2,[0],[1.0]) |\n",
       "| (1,[0],[1.0]) | (2,[0],[1.0]) | (2,[0],[1.0]) | (8,[0],[1.0]) | (39,[0],[1.0]) | (5,[0],[1.0]) | (3,[],[])     | (11,[6],[1.0]) | (3,[1],[1.0]) | (2,[0],[1.0]) |\n",
       "+---------------+---------------+---------------+---------------+----------------+---------------+---------------+----------------+---------------+---------------+"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_indexed.select(\"Sex_index_encoded\", \"fellow_type_index_encoded\", \"Embarked_index_encoded\", \"Cabin_index_encoded\", \"Ticket_index_encoded\", \"Title_index_encoded\", \"Pclass_encoded\", \"Age_categorized_encoded\", \"Fare_categorized_encoded\", \"family_type_encoded\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|prediction|         probability|       rawPrediction|\n",
      "+----------+--------------------+--------------------+\n",
      "|       0.0|[0.81312048921093...|[16.2624097842186...|\n",
      "|       1.0|[0.06949345575278...|[1.38986911505578...|\n",
      "|       1.0|[0.42792627436304...|[8.55852548726090...|\n",
      "|       1.0|[0.11751845835005...|[2.35036916700102...|\n",
      "|       0.0|[0.85582635592717...|[17.1165271185434...|\n",
      "|       0.0|[0.83015715429849...|[16.6031430859699...|\n",
      "|       1.0|[0.46082803361221...|[9.21656067224423...|\n",
      "|       0.0|[0.70185081605966...|[14.0370163211933...|\n",
      "|       1.0|[0.23987472789878...|[4.79749455797566...|\n",
      "|       1.0|[0.16763934116505...|[3.35278682330107...|\n",
      "|       1.0|[0.27560241875773...|[5.51204837515475...|\n",
      "|       1.0|[0.29037471227365...|[5.80749424547318...|\n",
      "|       0.0|[0.83574358951282...|[16.7148717902565...|\n",
      "|       0.0|[0.83795243577368...|[16.7590487154736...|\n",
      "|       1.0|[0.45964589961478...|[9.19291799229568...|\n",
      "|       1.0|[0.29587784072317...|[5.91755681446352...|\n",
      "|       0.0|[0.64610357115548...|[12.9220714231096...|\n",
      "|       0.0|[0.81819473041635...|[16.3638946083271...|\n",
      "|       0.0|[0.68907909551098...|[13.7815819102196...|\n",
      "|       1.0|[0.19862958209383...|[3.97259164187678...|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_indexed.select(\"prediction\",\"probability\",\"rawPrediction\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "// val paramGrid = new ParamGridBuilder()\n",
    "//   .addGrid(rfc.numTrees, Seq(50, 100, 150, 200))\n",
    "//   .addGrid(rfc.maxDepth, Seq(4, 5, 7, 10))\n",
    "//   .build()\n",
    "\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(gbtc.stepSize, Seq(0.03, 0.07, 0.13, 0.17))\n",
    "  .addGrid(gbtc.maxDepth, Seq(4, 5, 7, 10))\n",
    "  .addGrid(gbtc.maxIter, Seq(150, 200))\n",
    "  .build()\n",
    "\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "val evaluator = new BinaryClassificationEvaluator()\n",
    "  .setMetricName(\"areaUnderROC\")\n",
    "  .setRawPredictionCol(\"prediction\")\n",
    "  .setLabelCol(\"Survived\")\n",
    "\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "val multiclassEval = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"Survived\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "\n",
    "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
    "val tvs = new TrainValidationSplit()\n",
    "  .setTrainRatio(0.9) \n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(multiclassEval)\n",
    "\n",
    "//    val pipeline = new Pipeline().setStages(Array(assembler, indexer, classifier))\n",
    "\n",
    "//     val paramGrid = new ParamGridBuilder().\n",
    "//       addGrid(classifier.minInfoGain, Seq(0.0, 0.05)).\n",
    "//       addGrid(classifier.numTrees, Seq(1, 10)).\n",
    "//       build()\n",
    "\n",
    "//     val multiclassEval = new MulticlassClassificationEvaluator().\n",
    "//       setLabelCol(\"Cover_Type\").\n",
    "//       setPredictionCol(\"prediction\").\n",
    "//       setMetricName(\"accuracy\")\n",
    "\n",
    "//     val validator = new TrainValidationSplit().\n",
    "//       setSeed(Random.nextLong()).\n",
    "//       setEstimator(pipeline).\n",
    "//       setEvaluator(multiclassEval).\n",
    "//       setEstimatorParamMaps(paramGrid).\n",
    "//       setTrainRatio(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "validatorModel = tvs_a07660574bcd\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tvs_a07660574bcd"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val validatorModel = tvs.fit(df_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.classification.GBTClassifier\n",
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.{TrainValidationSplit, TrainValidationSplitModel}\n",
    "\n",
    "def trainData(df: Dataset[Row], vectorCols: Array[String]): TrainValidationSplitModel = {\n",
    "    //separate and model pipeline，包含划分label和features，机器学习模型的pipeline\n",
    "    val vectorAssembler = new VectorAssembler()\n",
    "      .setInputCols(vectorCols)\n",
    "      .setOutputCol(\"features\")\n",
    "    \n",
    "    val gbtc = new GBTClassifier()\n",
    "        .setLabelCol(\"Survived\")\n",
    "        .setFeaturesCol(\"features\")\n",
    "        .setPredictionCol(\"prediction\")\n",
    "    \n",
    "    val pipeline = new Pipeline().setStages(Array(vectorAssembler, gbtc))\n",
    "    \n",
    "    val paramGrid = new ParamGridBuilder()\n",
    "    .addGrid(gbtc.stepSize, Seq(0.03, 0.07, 0.13))\n",
    "    .addGrid(gbtc.maxDepth, Seq(5, 7, 10))\n",
    "    .addGrid(gbtc.maxIter, Seq(150, 200))\n",
    "    .build()\n",
    "    \n",
    "    val multiclassEval = new MulticlassClassificationEvaluator()\n",
    "      .setLabelCol(\"Survived\")\n",
    "      .setPredictionCol(\"prediction\")\n",
    "      .setMetricName(\"accuracy\")\n",
    "    \n",
    "    val tvs = new TrainValidationSplit()\n",
    "      .setTrainRatio(0.9) \n",
    "      .setEstimatorParamMaps(paramGrid)\n",
    "      .setEstimator(pipeline)\n",
    "      .setEvaluator(multiclassEval)\n",
    "    \n",
    "    tvs.fit(df)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "validatorModel = tvs_e366fa3a7918\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tvs_e366fa3a7918"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val validatorModel = trainData(df_indexed, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\trfc_2715e7422000-cacheNodeIds: false,\n",
      "\trfc_2715e7422000-checkpointInterval: 10,\n",
      "\trfc_2715e7422000-featureSubsetStrategy: auto,\n",
      "\trfc_2715e7422000-featuresCol: features,\n",
      "\trfc_2715e7422000-impurity: gini,\n",
      "\trfc_2715e7422000-labelCol: Survived,\n",
      "\trfc_2715e7422000-maxBins: 32,\n",
      "\trfc_2715e7422000-maxDepth: 7,\n",
      "\trfc_2715e7422000-maxMemoryInMB: 256,\n",
      "\trfc_2715e7422000-minInfoGain: 0.0,\n",
      "\trfc_2715e7422000-minInstancesPerNode: 1,\n",
      "\trfc_2715e7422000-numTrees: 100,\n",
      "\trfc_2715e7422000-predictionCol: prediction,\n",
      "\trfc_2715e7422000-probabilityCol: probability,\n",
      "\trfc_2715e7422000-rawPredictionCol: rawPrediction,\n",
      "\trfc_2715e7422000-seed: 207336481,\n",
      "\trfc_2715e7422000-subsamplingRate: 1.0\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bestModel = pipeline_e7d6af4e8350\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_e7d6af4e8350"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.PipelineModel\n",
    "val bestModel = validatorModel.bestModel\n",
    "println(bestModel.asInstanceOf[PipelineModel].stages.last.extractParamMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val paramsAndMetrics = validatorModel.validationMetrics\n",
    "  .zip(validatorModel.getEstimatorParamMaps)\n",
    "  .sortBy(-_._1)\n",
    "\n",
    "paramsAndMetrics.foreach { case (metric, params) => \n",
    "    println(metric)\n",
    "    println(params)\n",
    "    println() \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel.write.overwrite().save(\"Titanic_gbtc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark_2.2.2 - Scala",
   "language": "scala",
   "name": "spark_2.2.2_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
