{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Log Analysis\n",
    "\n",
    "1. 提取需要的log信息，包括time, traffic, ip, web adress\n",
    "2. 进一步解析第一步获得的log信息，如把ip转换为对应的省份，从网址中提取出访问内容和内容ID，最后将信息转换为parquet格式。\n",
    "3.  \n",
    "(1)按日期和内容（video）的ID进行分组，并根据访问次数进行倒序排序。  \n",
    "(2)按日期，内容（video）的ID和省份进行分组，并根据访问次数排名取前3。  \n",
    "最后将（1）和（2）数据写入MySQL。  \n",
    "\n",
    "注意：（1）写入数据库时分partition写入，而非逐条写入。  \n",
    "（2）先filter出公用的df并进行cache  \n",
    "（3）下面代码应该能进一步优化，例如将videoAccessTopNStat的try/catch中生成partition list和StatDAO.inserDayVideoAccessTopN(list)中生成batch应该可以合并，避免两次遍历。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设计和编写思路：  \n",
    "1.设计输入参数args（如inputPath和outputPath）  \n",
    "2.设计转换的工具类，包括StructType（需要提取什么信息，分别是什么格式），parseLog（split并提取各index的信息，用try/catch包裹，设置默认输出）。其中对时间的提取可另外定义一个工具类，包括inputFormat，outputFormat，getTime和parse。而对地域的提取，可另外定义一个IpUtils，引入开源代码ipdatabase。这些工具类写完后都要在自身main方法中测试。最后生成DF。  \n",
    "3.filter出commonDF。  \n",
    "4.实现特定的数据统计  \n",
    "5.输出数据，如果写入MySQL，就另外创建一个StatDAO类，包括获取链接，分批写入数据和release链接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Step One:\n",
    "\n",
    "/**\n",
    "  * 将原始日志数据进行解析，返回信息包括visit time, url, traffic, ip\n",
    "  * @param .log, example: 183.162.52.7 - - [10/Nov/2016:00:01:02 +0800] \n",
    "  * \"POST /api3/getadv HTTP/1.1\" ...\n",
    "  * @return partitioned files, example: 1970-01-01 08:00:00\\t-\n",
    "  * \\t813\\t183.162.52.7\n",
    "  */\n",
    "\n",
    "if (args.length != 2) {\n",
    "  println(\"Usage: logCleanYarn <inputPath> <outputPath>\")\n",
    "  System.exit(1)\n",
    "}\n",
    "\n",
    "val Array(inputPath, outputPath) = args\n",
    "\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "\n",
    "val access = spark.sparkContext.textFile(inputPath)\n",
    "\n",
    "//access.take(10).foreach(println)\n",
    "\n",
    "val splited = access.map(line => {\n",
    "\n",
    "   val splits = line.split(\" \")\n",
    "   val ip = splits(0)\n",
    "   val time = splits(3) + \" \" + splits(4)\n",
    "   val url = splits(11).replaceAll(\"\\\"\", \"\") //remove quotation mark\n",
    "   val traffic = splits(9)\n",
    "// (ip, DataUtils.parse(time), url, traffic)\n",
    "\n",
    "   DataUtils.parse(time) + \"\\t\" + url + \"\\t\" + traffic + \"\\t\" + ip\n",
    "    })\n",
    "\n",
    "splited.saveAsTextFile(outputPath)\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "/**\n",
    "  * 用于解析日志时间\n",
    "  */\n",
    "object DataUtils {\n",
    "\n",
    "  //input_format: [10/Nov/2016:00:01:02 +0800]\n",
    "  val YYYYMMDDHHMM_TIME_FORMAT = FastDateFormat.getInstance(\"dd/MMM/yyyy:HH:mm:SS Z\", Locale.ENGLISH)\n",
    "\n",
    "  //output_format: yyyy-MM-dd HH:mm:ss\n",
    "  val TARGET_FORMAT = FastDateFormat.getInstance(\"yyyy-MM-dd HH:mm:ss\")\n",
    "\n",
    "  def getTime(time: String) = {\n",
    "    try {\n",
    "      YYYYMMDDHHMM_TIME_FORMAT.parse(time.substring(time.indexOf(\"[\") + 1, time.lastIndexOf(\"]\"))).getTime\n",
    "    } catch {\n",
    "      case _ => 0l\n",
    "    }\n",
    "  }\n",
    "    \n",
    "  /**\n",
    "  * example: [10/Nov/2016:00:01:02 +0800] ==> 2016-11-10 00:01:00\n",
    "  */\n",
    "  def parse(time: String) = {\n",
    "    TARGET_FORMAT.format(new Date(getTime(time)))\n",
    "  }\n",
    "\n",
    "//  def main(args: Array[String]): Unit = {\n",
    "//    println(parse(\"[10/Nov/2016:00:01:02 +0800]\"))\n",
    "//  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Step Two:\n",
    "\n",
    "/**\n",
    "  * 将第一步解析出来的数据转化为DataFrame，并保存为一份parquet文件。\n",
    "  */\n",
    "\n",
    "if (args.length != 2) {\n",
    "  println(\"Usage: logCleanYarn <inputPath> <outputPath>\")\n",
    "  System.exit(1)\n",
    "}\n",
    "\n",
    "val Array(inputPath, outputPath) = args\n",
    "\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "\n",
    "val access = spark.sparkContext.textFile(inputPath)\n",
    "\n",
    "// access.take(10).foreach(println)\n",
    "\n",
    "val accessDF = spark.createDataFrame(access.map(line => AccessConvertUtil.parseLog(line)), AccessConvertUtil.struct)\n",
    "\n",
    "// accessDF.printSchema()\n",
    "// accessDF.show(false)\n",
    "\n",
    "accessDF.coalesce(1).write.format(\"parquet\").partitionBy(\"day\")\n",
    "      .save(outputPath)\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "/**\n",
    "  * 工具类，定义了schema和进一步解析log的方法\n",
    "  */\n",
    "object AccessConvertUtil {\n",
    "\n",
    "  val struct = StructType(Seq(\n",
    "    StructField(\"url\", StringType),\n",
    "    StructField(\"cmsType\", StringType),\n",
    "    StructField(\"cmsId\", IntegerType),\n",
    "    StructField(\"traffic\", IntegerType),\n",
    "    StructField(\"ip\", StringType),\n",
    "    StructField(\"city\", StringType),\n",
    "    StructField(\"time\", StringType),\n",
    "    StructField(\"day\", StringType)\n",
    "  ))\n",
    "\n",
    "  /**\n",
    "    * 进一步解析log，如转化数据类型，解析网址，ip映射具体省份，最后以Row输出\n",
    "    */\n",
    "  def parseLog(log: String) = {\n",
    "\n",
    "    try{\n",
    "      val splited = log.split(\"\\t\")\n",
    "\n",
    "      val url = splited(1)\n",
    "      val traffic = splited(2).toInt\n",
    "      val ip = splited(3)\n",
    "\n",
    "      // 网址：\"http://www.xxx.com/article/101\"中article为网页内容，101为article的ID\n",
    "      val domain = \"http://www.xxx.com/\"\n",
    "      val cms = url.substring(url.indexOf(domain) + domain.length)\n",
    "      val cmsTypeId = cms.split(\"/\")\n",
    "\n",
    "      var cmsType = \"\"\n",
    "      var cmsId = 0\n",
    "      if (cmsTypeId.length > 1) {\n",
    "        cmsType = cmsTypeId(0)\n",
    "        cmsId = cmsTypeId(1).toInt\n",
    "      }\n",
    "\n",
    "      val city = IpUtils.getCity(ip)\n",
    "      val time = splited(0)\n",
    "      val day = time.substring(0, 10).replaceAll(\"-\", \"\")\n",
    "\n",
    "      Row(url, cmsType, cmsId, traffic, ip, city, time, day)\n",
    "    } catch {\n",
    "      case _ => {\n",
    "        Row(null, null, null, null, null, null, null, null)\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "/**\n",
    "  * Ip工具类，将IP映射为省份，利用开源代码ipdatabase\n",
    "  * https://github.com/wzhe06/ipdatabase\n",
    "  */\n",
    "object IpUtils {\n",
    "\n",
    "  def getCity(ip: String) = {\n",
    "    IpHelper.findRegionByIp(ip)\n",
    "  }\n",
    "\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    println(getCity(\"58.30.15.255\"))\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Step Three:\n",
    "\n",
    "/**\n",
    "  * 在第二步的结果数据中，按日期和video的ID进行分组，并根据访问次数进行倒序排序。\n",
    "  * 最后将数据写入MySQL。\n",
    "  */\n",
    "\n",
    "if (args.length != 2) {\n",
    "  println(\"Usage: logCleanYarn <inputPath> <day>\")\n",
    "  System.exit(1)\n",
    "}\n",
    "\n",
    "val Array(inputPath, day) = args\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .config(\"spark.sql.sources.partitionColumnTypeInference.enabled\", \"false\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val accessDF = spark.read.format(\"parquet\").load(inputPath)\n",
    "\n",
    "//    accessDF.printSchema()\n",
    "//    accessDF.show(false)\n",
    "\n",
    "//预先筛选和cache后面两个函数要复用的df\n",
    "import spark.implicits._\n",
    "val commonDF = accessDF.filter($\"day\" === day && $\"cmsType\" === \"video\")\n",
    "commonDF.cache()\n",
    "\n",
    "//删除已有的内容，避免重复\n",
    "StatDAO.deleteData(day)\n",
    "\n",
    "//groupBy video\n",
    "videoAccessTopNStat(spark, commonDF)\n",
    "\n",
    "//groupBy city\n",
    "cityAccessTopNStat(spark, commonDF)\n",
    "\n",
    "commonDF.unpersist(true)\n",
    "\n",
    "//    videoAccessTopDF.show(false)\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "/**\n",
    "  * 两个样例类，用于储存不同数据类型，应用于下面两个方法。\n",
    "  */\n",
    "case class DayVideoAccessStat(day: String, cmsId: Long, times: Long)\n",
    "case class DayCityVideoAccessStat(day: String, cmsId: Long, city: String, times: Long, timesRank: Int)\n",
    "\n",
    "/**\n",
    "  * 按内容ID分组后排序，并把结果写到Mysql\n",
    "  */\n",
    "def videoAccessTopNStat(spark: SparkSession, comDF: DataFrame): Unit = {\n",
    "\n",
    "  import spark.implicits._\n",
    "  val videoAccessTopNStat = comDF\n",
    "    .groupBy($\"day\", $\"cmsId\")\n",
    "    .agg(count(\"cmsId\").as(\"times\"))\n",
    "    .orderBy(desc(\"times\"))\n",
    "\n",
    "  try {\n",
    "    videoAccessTopNStat.foreachPartition(partitionOfRecords =>{\n",
    "      val list = new ListBuffer[DayVideoAccessStat]\n",
    "\n",
    "      partitionOfRecords.foreach(info => {\n",
    "        val day = info.getAs[String](\"day\")\n",
    "        val cmsId = info.getAs[Long](\"cmsId\")\n",
    "        val times = info.getAs[Long](\"times\")\n",
    "\n",
    "        list.append(DayVideoAccessStat(day, cmsId, times))\n",
    "      })\n",
    "\n",
    "      StatDAO.inserDayVideoAccessTopN(list)\n",
    "    })\n",
    "  } catch {\n",
    "    case e:Exception => e.printStackTrace()\n",
    "  }\n",
    "}\n",
    "\n",
    "/**\n",
    "  * 按内容ID和省份分组后排名，并把结果写到Mysql\n",
    "  */\n",
    "def cityAccessTopNStat(spark: SparkSession, comDF: DataFrame): Unit = {\n",
    "\n",
    "  import spark.implicits._\n",
    "\n",
    "  val videoAccessTopNStat = comDF\n",
    "    .groupBy($\"day\", $\"city\", $\"cmsId\")\n",
    "    .agg(count(\"cmsId\").as(\"times\"))\n",
    "\n",
    "  val windowSpec = Window.partitionBy($\"city\").orderBy(desc(\"times\"))\n",
    "  val videoAccessTopNStatDF = videoAccessTopNStat.select(expr(\"*\"), rank().over(windowSpec).as(\"times_rank\"))\n",
    "    .filter($\"times_rank\" <= 3)\n",
    "\n",
    "  try {\n",
    "    videoAccessTopNStatDF.foreachPartition(partitionOfRecords => {\n",
    "      val list = new ListBuffer[DayCityVideoAccessStat]\n",
    "\n",
    "      partitionOfRecords.foreach(info => {\n",
    "        val day = info.getAs[String](\"day\")\n",
    "        val cmsId = info.getAs[Long](\"cmsId\")\n",
    "        val city = info.getAs[String](\"city\")\n",
    "        val times = info.getAs[Long](\"times\")\n",
    "        val timesRank = info.getAs[Int](\"times_rank\")\n",
    "\n",
    "        list.append(DayCityVideoAccessStat(day, cmsId, city, times, timesRank))\n",
    "      })\n",
    "\n",
    "      StatDAO.inserDayCityVideoAccessTopN(list)\n",
    "    })\n",
    "  } catch {\n",
    "    case e: Exception => e.printStackTrace()\n",
    "  }\n",
    "}\n",
    "\n",
    "/**\n",
    "  * 分组后排序方法\n",
    "  */\n",
    "def videoAccessSortedStat(spark: SparkSession, accessDF: DataFrame) : Unit = {\n",
    "  import spark.implicits._\n",
    "    \n",
    "  val sortedStat= accessDF\n",
    "    .filter($\"day\" === \"20170511\" && $\"cmsType\" === \"video\")\n",
    "    .groupBy($\"day\", $\"cmsId\")\n",
    "    .agg(count(\"cmsId\").as(\"times\"))\n",
    "    .orderBy(desc(\"times\"))\n",
    "    \n",
    "  // 分块创建存储每条信息的list，并调用函数将数据写到到MySQL\n",
    "  try {\n",
    "      sortedStat.foreachPartition(partitionOfRecords =>{\n",
    "        val list = new ListBuffer[DayVideoAccessStat]\n",
    "\n",
    "        partitionOfRecords.foreach(info => {\n",
    "          val day = info.getAs[String](\"day\")\n",
    "          val cmsId = info.getAs[Long](\"cmsId\")\n",
    "          val times = info.getAs[Long](\"times\")\n",
    "\n",
    "          list.append(DayVideoAccessStat(day, cmsId, times))\n",
    "        })\n",
    "\n",
    "        StatDAO.inserDayVideoAccessSortedStat(list)\n",
    "      })\n",
    "   } catch {\n",
    "  case e:Exception => e.printStackTrace()\n",
    " }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Step Three:\n",
    "\n",
    "/**\n",
    "  * 工具类，提供两类方法：\n",
    "  * 1.连接数据库，将数据写入MySQL，并释放连接的方法。\n",
    "  * 2.删除MySQL中已存在的（相同entry的数据）\n",
    "  */\n",
    "object StatDAO {\n",
    "\n",
    "  def inserDayVideoAccessTopN(list: ListBuffer[DayVideoAccessStat]): Unit = {\n",
    "\n",
    "    var connection: Connection = null\n",
    "    var pstmt: PreparedStatement = null\n",
    "\n",
    "    try{\n",
    "      connection = MySQLUtils.getConnect()\n",
    "\n",
    "      val sql = \"insert into day_video_access_topn_stat(day, cms_id, times) values (?, ?, ?)\"\n",
    "      val pstmt = connection.prepareStatement(sql)\n",
    "\n",
    "      connection.setAutoCommit(false)\n",
    "\n",
    "      for (ele <- list) {\n",
    "        pstmt.setString(1, ele.day)\n",
    "        pstmt.setLong(2, ele.cmsId)\n",
    "        pstmt.setLong(3, ele.times)\n",
    "\n",
    "        pstmt.addBatch()\n",
    "      }\n",
    "\n",
    "      pstmt.executeBatch()\n",
    "      connection.commit()\n",
    "\n",
    "    } catch {\n",
    "      case e:Exception => e.printStackTrace()\n",
    "    } finally {\n",
    "      MySQLUtils.release(connection, pstmt)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def inserDayCityVideoAccessTopN(list: ListBuffer[DayCityVideoAccessStat]): Unit = {\n",
    "\n",
    "    var connection: Connection = null\n",
    "    var pstmt: PreparedStatement = null\n",
    "\n",
    "    try{\n",
    "      connection = MySQLUtils.getConnect()\n",
    "\n",
    "      val sql = \"insert into day_video_city_access_topn_stat(day, cms_id, city, times, times_rank) values (?, ?, ?, ?, ?)\"\n",
    "      val pstmt = connection.prepareStatement(sql)\n",
    "\n",
    "      connection.setAutoCommit(false)\n",
    "\n",
    "      for (ele <- list) {\n",
    "        pstmt.setString(1, ele.day)\n",
    "        pstmt.setLong(2, ele.cmsId)\n",
    "        pstmt.setString(3, ele.city)\n",
    "        pstmt.setLong(4, ele.times)\n",
    "        pstmt.setInt(5, ele.timesRank)\n",
    "\n",
    "        pstmt.addBatch()\n",
    "      }\n",
    "\n",
    "      pstmt.executeBatch()\n",
    "      connection.commit()\n",
    "\n",
    "    } catch {\n",
    "      case e:Exception => e.printStackTrace()\n",
    "    } finally {\n",
    "      MySQLUtils.release(connection, pstmt)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def deleteData(day: String): Unit = {\n",
    "\n",
    "    val tables = Array(\"day_video_access_topn_stat\", \"day_video_city_access_topn_stat\")\n",
    "    var connection: Connection = null\n",
    "    var pstmt: PreparedStatement = null\n",
    "\n",
    "    try {\n",
    "      connection = MySQLUtils.getConnect()\n",
    "\n",
    "      for (table <- tables) {\n",
    "        val sql = s\"delete from $table where day = ?\"\n",
    "        val pstmt = connection.prepareStatement(sql)\n",
    "        pstmt.setString(1, day)\n",
    "        pstmt.executeUpdate()\n",
    "\n",
    "      }\n",
    "    } catch {\n",
    "      case e: Exception => e.printStackTrace()\n",
    "    } finally {\n",
    "      MySQLUtils.release(connection, pstmt)\n",
    "    }\n",
    "\n",
    "  }\n",
    "}\n",
    "\n",
    "/**\n",
    "  * 工具类，包含连接数据库和释放连接的方法。\n",
    "  */\n",
    "object MySQLUtils {\n",
    "\n",
    "  def getConnect() = {\n",
    "      DriverManager.getConnection(\"jdbc:mysql://localhost:3306/log_project\",\"root\", \"password\")\n",
    "  }\n",
    "\n",
    "  def release(connection: Connection, pstmt: PreparedStatement): Unit ={\n",
    "    try{\n",
    "      if (pstmt != null) {\n",
    "        pstmt.close()\n",
    "      }\n",
    "    } catch {\n",
    "      case e: Exception => e.printStackTrace()\n",
    "    } finally {\n",
    "      if (connection != null) {\n",
    "        connection.close()\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    println(getConnect())\n",
    "  }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark_2.2.2 - Scala",
   "language": "scala",
   "name": "spark_2.2.2_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
