{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Time Streaming Process\n",
    "本案例由python脚本产生网站日志，通过flume和kafka将数据传递给Spark Streaming，通过计算后写入HBase。  \n",
    "\n",
    "1.设置好KafkaUtils.createStream所需参数  \n",
    "2.日志的信息提取和转换，创建一个case class来存储每条信息  \n",
    "3.数据统计并写入数据库  \n",
    "\n",
    "注意：  \n",
    "（1）incrementColumnValue并非一个好的写入方式，应改用批量put  \n",
    "（2）实际生产中可能更多用KafkaUtils.createDirectStream，但Spark代码的变化不大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (args.length != 4) {\n",
    "  System.err.println(\"Usage: FlumePushWC <zkQuorum> <group> <topics> <numTreads>\")\n",
    "  System.exit(1)\n",
    "}\n",
    "\n",
    "val Array(zkQuorum, group, topics, numTreads) = args\n",
    "\n",
    "val conf = new SparkConf() //.setMaster(\"local[2]\").setAppName(\"streamingAPP\")\n",
    "\n",
    "val ssc = new StreamingContext(conf, Seconds(60))\n",
    "\n",
    "//topicMap能映射每个topics给相应的线程数\n",
    "val topicMap = topics.split(\",\").map((_, numTreads.toInt)).toMap\n",
    "\n",
    "val kafkaStream = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap)\n",
    "//kafkaStream.map(_._2).flatMap(_.split(\" \")).map((_, 1)).reduceByKey(_ + _).print()\n",
    "\n",
    "//kafkaStream的第二位为信息本体\n",
    "//143.132.29.124\t2018-08-23 12:22:00\t\"GET /class/116.html HTTP/1.1\"\t200\thttp://www.sogou.com/web?query=Hadoop基础\n",
    "val cleanData = kafkaStream.map(_._2)\n",
    "  .map(line => {\n",
    "    val info = line.split(\"\\t\")\n",
    "\n",
    "    val url = info(2).split(\" \")(1)\n",
    "    var courseId = 0\n",
    "\n",
    "    if (url.startsWith(\"/class\")) {\n",
    "      val courseIdHTML = url.split(\"/\")(2)\n",
    "      //此处不能加val，否则就是创建新值，无法对if外部的courseId作修改\n",
    "      courseId = courseIdHTML.substring(0, courseIdHTML.lastIndexOf(\".\")).toInt\n",
    "    }\n",
    "\n",
    "    //存储信息的case class\n",
    "    ClickLog(info(0), DateUtils.parseToMinute(info(1)), courseId, info(3).toInt, info(4))\n",
    "  }).filter(clicklog => clicklog.courseId != 0)\n",
    "\n",
    "// cleanData.print()\n",
    "\n",
    "//先生成rowkey和相应的value，然后聚合，再将数据写入Hbase。\n",
    "//CourseClickCount为case class\n",
    "//CourseClickCoutDAO看下下一个代码块\n",
    "cleanData.map(x => {\n",
    "  (x.time.substring(0, 8) + \"_\" + x.courseId, 1)\n",
    "})\n",
    "  .reduceByKey(_+_)\n",
    "  .foreachRDD(rdd => {\n",
    "  rdd.foreachPartition(partitionRecords => {\n",
    "    val list = new ListBuffer[CourseClickCount]\n",
    "    partitionRecords.foreach(pair => {\n",
    "      list.append(CourseClickCount(pair._1, pair._2))\n",
    "    })\n",
    "    CourseClickCoutDAO.save(list)\n",
    "  })\n",
    "})\n",
    "\n",
    "cleanData.map(x => {\n",
    "  //https://cn.bing.com/search?q=Spark\n",
    "  val refer = x.ref.replaceAll(\"//\", \"/\")\n",
    "  val splits = refer.split(\"/\")\n",
    "  var host = \"\"\n",
    "\n",
    "  if (splits.length > 2) {\n",
    "    host = splits(1)\n",
    "  }\n",
    "  (host, x.courseId, x.time)\n",
    "}).filter(_._1 != \"\")\n",
    "  .map(x => (x._3.substring(0, 8) + \"_\" + x._1 + \"_\" + x._2, 1))\n",
    "  .reduceByKey(_+_).foreachRDD(rdd => {\n",
    "  rdd.foreachPartition(partitionRecords => {\n",
    "    val list = new ListBuffer[CourseSearchClickCount]\n",
    "    partitionRecords.foreach(pair => {\n",
    "      list.append(CourseSearchClickCount(pair._1, pair._2))\n",
    "    })\n",
    "    CourseSearchClickCoutDAO.save(list)\n",
    "  })\n",
    "})\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//解析时间的工具类\n",
    "object DateUtils {\n",
    "\n",
    "  val YYYYMMDDHHMMSS_FORMAT = FastDateFormat.getInstance(\"yyyy-MM-dd HH:mm:ss\")\n",
    "  val TARGE_FORMAT = FastDateFormat.getInstance(\"yyyyMMddHHmmss\")\n",
    "\n",
    "  def getTime(time: String) = {\n",
    "    YYYYMMDDHHMMSS_FORMAT.parse(time).getTime\n",
    "  }\n",
    "\n",
    "  def parseToMinute(time: String) = {\n",
    "    TARGE_FORMAT.format(new Date(getTime(time)))\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//此单例类包括save和query方法。\n",
    "object CourseClickCoutDAO {\n",
    "  private val tableName = \"course_clickcount\"\n",
    "  private val columnfamily = \"info\"\n",
    "  private val qualifer = \"click_count\"\n",
    "\n",
    "  def save(list: ListBuffer[CourseClickCount]): Unit ={\n",
    "    val table = HBaseUtils.getInstance().getTable(tableName)\n",
    "\n",
    "    for (elem <- list) {\n",
    "      //此处改为批量put效率更好\n",
    "      table.incrementColumnValue(Bytes.toBytes(elem.day_course),\n",
    "        Bytes.toBytes(columnfamily),\n",
    "        Bytes.toBytes(qualifer),\n",
    "        elem.click_count)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def queryByKeyRow(day_couse: String): Long ={\n",
    "    val table = HBaseUtils.getInstance().getTable(tableName)\n",
    "    val get = new Get(Bytes.toBytes(day_couse))\n",
    "    val value = table.get(get).getValue(columnfamily.getBytes(), qualifer.getBytes())\n",
    "\n",
    "    if (value == null) {\n",
    "      0l\n",
    "    } else {\n",
    "      Bytes.toLong(value)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def main(args: Array[String]): Unit = {\n",
    "\n",
    "    val list = new ListBuffer[CourseClickCount]\n",
    "    list.append(CourseClickCount(\"20171111_8\", 8))\n",
    "    list.append(CourseClickCount(\"20171111_9\", 9))\n",
    "    list.append(CourseClickCount(\"20171111_1\", 100))\n",
    "\n",
    "    save(list)\n",
    "    println(queryByKeyRow(\"20171111_8\") + \":\" +\n",
    "      queryByKeyRow(\"20171111_9\") + \":\" +\n",
    "      queryByKeyRow(\"20171111_1\"))\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//下面为Java实现的Hbase工作类模版\n",
    "public class HBaseUtils {\n",
    "\n",
    "    HBaseAdmin admin = null;\n",
    "    Configuration conf = null;\n",
    "\n",
    "    private HBaseUtils() {\n",
    "        conf = new Configuration();\n",
    "        conf.set(\"hbase.zookeeper.quorum\", \"localhost:2181\");\n",
    "        conf.set(\"hbase.rootdir\", \"hdfs://localhost:8020/hbase\");\n",
    "\n",
    "        try {\n",
    "            admin = new HBaseAdmin(conf);\n",
    "        } catch (IOException e) {\n",
    "            e.printStackTrace();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    private static HBaseUtils instance = null;\n",
    "    \n",
    "    //利用getInstance来实例化HBaseUtils，加上synchronized来保证本类只有一个实例化对象\n",
    "    public static synchronized HBaseUtils getInstance() {\n",
    "        if (null == instance) {\n",
    "            instance = new HBaseUtils();\n",
    "        }\n",
    "        return instance;\n",
    "    }\n",
    "\n",
    "    //取得Hbase的表格\n",
    "    public HTable getTable(String tableName) {\n",
    "        HTable table = null;\n",
    "        try {\n",
    "            table = new HTable(conf, tableName);\n",
    "        } catch (IOException e) {\n",
    "            e.printStackTrace();\n",
    "        }\n",
    "        return table;\n",
    "    }\n",
    "\n",
    "    //把新值新增/累加到Hbase中\n",
    "    private void put(String tableName, String rowkey, String columnfamily, String column, String value) {\n",
    "        HTable table = getTable(tableName);\n",
    "        Put put = new Put(Bytes.toBytes(rowkey));\n",
    "        put.add(Bytes.toBytes(columnfamily), Bytes.toBytes(column), Bytes.toBytes(value));\n",
    "\n",
    "        try {\n",
    "            table.put(put);\n",
    "        } catch (IOException e) {\n",
    "            e.printStackTrace();\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark_2.2.2 - Scala",
   "language": "scala",
   "name": "spark_2.2.2_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
