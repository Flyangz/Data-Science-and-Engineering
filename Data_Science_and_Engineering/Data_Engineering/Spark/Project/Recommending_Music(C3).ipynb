{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本例子来自《Advanced Analytics with Spark 2nd》，需要在进入spark-shell时预先设置spark.driver.memory的大小，默认1G不够用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val spk = spark\n",
    "import spk.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//加载UserArtist（用户，作者和interaction count）数据\n",
    "val UserArtistPath = \"/Users/flyang/Documents/self-teaching/Data_Resources/profiledata_06-May-2005/user_artist_data.txt\"\n",
    "val rawUserArtistData = spark.read.textFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//分隔数据\n",
    "val userArtistDF = rawUserArtistData.map { line => \n",
    "    val Array(user, artist, _*) = line.split(' ') \n",
    "    (user.toInt, artist.toInt)\n",
    "}.toDF(\"user\", \"artist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//加载artist（id和名字）数据\n",
    "val artistPath = \"/Users/flyang/Documents/self-teaching/Data_Resources/profiledata_06-May-2005/artist_data.txt\"\n",
    "val rawArtistData = spark.read.textFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//分隔数据\n",
    "val artistByID = rawArtistData.flatMap { line => \n",
    "    val (id, name) = line.span(_ != '\\t')\n",
    "    try {\n",
    "        Some(id.toInt, name.trim)\n",
    "    } catch {\n",
    "        case _: Exception => None \n",
    "    }\n",
    "}.toDF(\"id\", \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//加载artistAlias（作者id和别id）数据\n",
    "val artistAliasPath = \"/Users/flyang/Documents/self-teaching/Data_Resources/profiledata_06-May-2005/artist_alias.txt\"\n",
    "val rawArtistAlias = spark.read.textFile(artistAliasPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//分隔数据\n",
    "val artistAlias = rawArtistAlias.flatMap { line =>\n",
    "    val Array(artist, alias) = line.split('\\t') \n",
    "    if (artist.isEmpty) {\n",
    "        None\n",
    "    }else{\n",
    "        Some(artist.toInt, alias.toInt)\n",
    "    }\n",
    "}.collect().toMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|     id|            name|\n",
      "+-------+----------------+\n",
      "|1208690|Collective Souls|\n",
      "|1003926| Collective Soul|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artistByID.filter($\"id\" isin (1208690, 1003926)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainData.type = [user: int, artist: int ... 1 more field]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.broadcast._\n",
    "\n",
    "//下面代码把有别名ID的作品统一成唯一ID\n",
    "def buildCounts(\n",
    "    rawUserArtistData: Dataset[String],\n",
    "    bArtistAlias: Broadcast[Map[Int,Int]]): DataFrame = {\n",
    "        rawUserArtistData.map { line =>\n",
    "            val Array(userID, artistID, count) = line.split(' ').map(_.toInt) \n",
    "            val finalArtistID =\n",
    "              bArtistAlias.value.getOrElse(artistID, artistID)\n",
    "            (userID, finalArtistID, count)\n",
    "        }.toDF(\"user\", \"artist\", \"count\")\n",
    "    }\n",
    "    \n",
    "val bArtistAlias = spark.sparkContext.broadcast(artistAlias) \n",
    "val trainData = buildCounts(rawUserArtistData, bArtistAlias)\n",
    "trainData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//尝试模型\n",
    "import org.apache.spark.ml.recommendation._\n",
    "import scala.util.Random\n",
    "val alsModel = new ALS().setSeed(Random.nextLong()).setImplicitPrefs(true).setRank(10).setRegParam(0.01).setAlpha(1.0).setMaxIter(5).setUserCol(\"user\").setItemCol(\"artist\").setRatingCol(\"count\").setPredictionCol(\"prediction\").fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//查看结果\n",
    "alsModel.recommendForAllUsers(10)\n",
    "  .selectExpr(\"userId\", \"explode(recommendations)\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark_2.2.2 - Scala",
   "language": "scala",
   "name": "spark_2.2.2_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
