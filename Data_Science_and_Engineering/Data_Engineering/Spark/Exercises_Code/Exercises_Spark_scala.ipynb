{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//prepare data\n",
    "case class Bili(id:String, mark:Integer, age:Integer)\n",
    "\n",
    "val info = sc.parallelize(Array(\n",
    "    Bili(\"a\", 80, 25), \n",
    "    Bili(\"a\", 90, 26), \n",
    "    Bili(\"a\", 90, 27),\n",
    "    Bili(\"b\", 98, 27),\n",
    "    Bili(\"b\", 92, 10),\n",
    "    Bili(\"b\", 82, 23),\n",
    "    Bili(\"b\", 82, 23),\n",
    "    Bili(\"b\", 80, 25),\n",
    "    Bili(\"b\", 84, 25),\n",
    "    Bili(\"c\", 84, 21),\n",
    "    Bili(\"c\", 84, 21),\n",
    "    Bili(\"c\", 80, 25)\n",
    "))\n",
    "\n",
    "val text = \"Apache Spark is a unified analytics engine for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//wordCount and sort\n",
    "val lines = sc.textFile(\"test.txt\")\n",
    "val words = lines.flatMap(_.split(\" \"))\n",
    "val pairs = words.map((_, 1))\n",
    "val count = pairs.reduceByKey(_+_)//对Tuple2的RDD进行隐式转换成PairRDDFunction，提供reduceByKey\n",
    "val sorted = count.sortBy(_._2, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//custom sorting all\n",
    "val result = info.sortBy{case Bili(id, mark, age) => (-mark, age)}\n",
    "\n",
    "// sorting in group when rdd has no partitioners\n",
    "import org.apache.spark.HashPartitioner\n",
    "\n",
    "case class Bili(id : Int, mark : Int) extends Ordered[Bili]{\n",
    "override def compare(that: Bili) = {\n",
    "  if (this.id != that.id){\n",
    "    this.id - that.id //id大的排前面\n",
    "  }else{\n",
    "    that.mark - this.mark //mark小的排前面\n",
    "  }\n",
    " }\n",
    "}\n",
    "\n",
    "val result = info.repartitionAndSortWithinPartitions(new HashPartitioner(n))\n",
    "\n",
    "// sorting in group when rdd already has partitioner\n",
    "val result = info.mapPartitions(_.sortBy{case Bili(id, mark, age) => (-mark, age)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//top n in RDD with agg\n",
    "import scala.math.Ordering\n",
    "import scala.collection.mutable.PriorityQueue\n",
    "\n",
    "implicit val ord = Ordering.by[Bili, String](_.id)\n",
    "val result = info.keyBy(_.id).\n",
    "                aggregateByKey(new scala.collection.mutable.PriorityQueue[Bili]())(\n",
    "                    (acc, v) => {\n",
    "                        acc.enqueue(v)\n",
    "                        acc.take(n)},\n",
    "                    (acc1, acc2) => (acc1 ++ acc2).take(n))\n",
    "\n",
    "//top n in RDD without agg\n",
    "rdd.top(n)(Ordering.by[Bili, String](_.id))\n",
    "\n",
    "//top n in dataframe\n",
    "val windowSpec = Window.partitionBy(\"category\").\n",
    "                        orderBy(desc(\"sale\"))\n",
    "windf.select(expr(\"*\"), rank().over(windowSpec)).\n",
    "      filter(col(\"b\") <= n).\n",
    "      show\n",
    "//top 1\n",
    "//stackoverflow.com/questions/33878370/how-to-select-the-first-row-of-each-group/33878701#33878701\n",
    "case class Record(Hour: Integer, Category: String, TotalValue: Double)\n",
    "df.as[Record]\n",
    "  .groupByKey(_.Hour)\n",
    "  .reduceGroups((x, y) => if (x.TotalValue > y.TotalValue) x else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//find the top n Searched Keyword with filter on daily basis\n",
    "//prepare DataFrame\n",
    "val myManualSchema =  StructType(Array(\n",
    "    StructField(\"Date\", DateType, true),\n",
    "    StructField(\"Name\", StringType, true),\n",
    "    StructField(\"Product\", StringType, true),\n",
    "    StructField(\"City\", StringType, false),\n",
    "    StructField(\"Platform\", StringType, false),\n",
    "    StructField(\"Times\", DoubleType, false)))\n",
    "val myRows = Seq(Row(Date.valueOf(\"2015-10-01\"), \"jack\", \"toy\", \"beijing\", \"android\", 1.5),\n",
    "           Row(Date.valueOf(\"2015-10-01\"), \"tom\", \"sea\", \"beijing\", \"android\", 1.0),\n",
    "           Row(Date.valueOf(\"2015-10-01\"), \"leo\", \"apple\", \"guangzhou\", \"android\", 1.5),\n",
    "           Row(Date.valueOf(\"2015-10-01\"), \"white\", \"toy\", \"beijing\", \"iphone\", 1.1),\n",
    "           Row(Date.valueOf(\"2015-10-01\"), \"jack\", \"toy\", \"beijing\", \"android\", 1.0),\n",
    "           Row(Date.valueOf(\"2015-10-01\"), \"leo\", \"sea\", \"beijing\", \"android\", 1.0),\n",
    "           Row(Date.valueOf(\"2015-10-01\"), \"tom\", \"toy\", \"beijing\", \"iphone\", 1.0),\n",
    "           Row(Date.valueOf(\"2015-10-01\"), \"may\", \"sea\", \"beijing\", \"android\", 1.5),\n",
    "           Row(Date.valueOf(\"2015-10-01\"), \"jack\", \"toy\", \"beijing\", \"android\", 1.0),\n",
    "           Row(Date.valueOf(\"2015-10-01\"), \"tom\", \"bar\", \"beijing\", \"android\", 1.5),\n",
    "           Row(Date.valueOf(\"2015-10-02\"), \"jack\", \"bar\", \"guangzhou\", \"android\", 1.1),\n",
    "           Row(Date.valueOf(\"2015-10-02\"), \"jack\", \"toy\", \"beijing\", \"android\", 1.0),\n",
    "           Row(Date.valueOf(\"2015-10-02\"), \"leo\", \"sea\", \"beijing\", \"iphone\", 1.5),\n",
    "           Row(Date.valueOf(\"2015-10-02\"), \"jack\", \"toy\", \"beijing\", \"android\", 1.0),\n",
    "           Row(Date.valueOf(\"2015-10-02\"), \"tom\", \"toy\", \"beijing\", \"android\", 2.0),\n",
    "           Row(Date.valueOf(\"2015-10-02\"), \"jack\", \"kk\", \"beijing\", \"android\", 1.0),\n",
    "           Row(Date.valueOf(\"2015-10-02\"), \"may\", \"water\", \"beijing\", \"android\", 1.0),\n",
    "           Row(Date.valueOf(\"2015-10-02\"), \"jack\", \"toy\", \"beijing\", \"android\", 1.0),\n",
    "           Row(Date.valueOf(\"2015-10-02\"), \"leo\", \"toy\", \"beijing\", \"android\", 2.0),\n",
    "           Row(Date.valueOf(\"2015-10-02\"), \"tom\", \"water\", \"beijing\", \"android\", 1.0),\n",
    "           Row(Date.valueOf(\"2015-10-02\"), \"may\", \"apple\", \"beijing\", \"android\", 2.0))\n",
    "val myRDD = spark.sparkContext.parallelize(myRows)\n",
    "val df = spark.createDataFrame(myRDD, myManualSchema)\n",
    "\n",
    "//filter records according to \"city\", \"platform\" and \"time\"\n",
    "val filtered = df.filter($\"City\" === \"beijing\" and $\"Platform\" === \"android\" and $\"Times\".isin(1.0,1.5,2.0))\n",
    "\n",
    "//def usf to count user visit\n",
    "spark.udf.register(\"count_uv\", (s: Seq[String]) => s.size)\n",
    "//exctually, we can use \"size\" sqlfunction\n",
    "\n",
    "//groupby \"Date\" and \"Product\", then counts the number of distinct people belong to these groups\n",
    "val agged = filtered.groupBy($\"Date\", $\"Product\").\n",
    "                    agg(collect_set(\"Name\").alias(\"unique\")).\n",
    "                    withColumn(\"UV\", expr(\"count_uv(unique)\"))\n",
    "                    \n",
    "//use window func to compute topn\n",
    "val windowSpec = Window.partitionBy($\"Date\").orderBy(desc(\"UV\"))\n",
    "val result = agged.withColumn(\"rank\", rank.over(windowSpec)).where($\"rank\" <= n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark_2.2.2 - Scala",
   "language": "scala",
   "name": "spark_2.2.2_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
